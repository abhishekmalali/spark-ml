{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serial Tree Creation, Parallel Forest Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin with a small sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample=[(1,[1,1,0,1]),\n",
    "         (0,[0,0,1,1]),\n",
    "         (1,[1,1,0,2]),\n",
    "         (0,[1,1,0,1]),\n",
    "         (0,[0,0,1,0]),\n",
    "         (2,[1,0,2,1]),\n",
    "         (2,[2,0,1,1]),\n",
    "         (2,[1,2,1,1]),\n",
    "         (0,[0,1,1,1]),\n",
    "         (1,[1,2,0,2]),\n",
    "         (0,[1,1,1,1]),\n",
    "         (0,[0,1,1,0]),\n",
    "         (2,[2,2,2,1]),\n",
    "         (2,[2,1,1,1]),\n",
    "         (2,[2,2,1,1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below was borrowed from and then adapted for our array of labeledPoints from: http://www-users.cs.umn.edu/~kumar/dmbook/ch4.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module holds functions that are responsible for creating a new\n",
    "decision tree and for using the tree for data classificiation.\n",
    "\"\"\"\n",
    "\n",
    "def majority_value(data):\n",
    "    \"\"\"\n",
    "    Creates a list of all values in the target attribute for each record\n",
    "    in the data list object, and returns the value that appears in this list\n",
    "    the most frequently.\n",
    "    \"\"\"\n",
    "    return most_frequent([record[0] for record in data])\n",
    "\n",
    "def most_frequent(lst):\n",
    "    \"\"\"\n",
    "    Returns the item that appears most frequently in the given list.\n",
    "    \"\"\"\n",
    "    lst = lst[:]\n",
    "    highest_freq = 0\n",
    "    most_freq = None\n",
    "\n",
    "    for val in unique(lst):\n",
    "        if lst.count(val) > highest_freq:\n",
    "            most_freq = val\n",
    "            highest_freq = lst.count(val)\n",
    "            \n",
    "    return most_freq\n",
    "\n",
    "def unique(lst):\n",
    "    \"\"\"\n",
    "    Returns a list made up of the unique values found in lst.  i.e., it\n",
    "    removes the redundant values in lst.\n",
    "    \"\"\"\n",
    "    lst = lst[:]\n",
    "    unique_lst = []\n",
    "\n",
    "    # Cycle through the list and add each value to the unique list only once.\n",
    "    for item in lst:\n",
    "        if unique_lst.count(item) <= 0:\n",
    "            unique_lst.append(item)\n",
    "            \n",
    "    # Return the list with all redundant values removed.\n",
    "    return unique_lst\n",
    "\n",
    "def get_values(data, attr):\n",
    "    \"\"\"\n",
    "    Creates a list of values in the chosen attribut for each record in data,\n",
    "    prunes out all of the redundant values, and return the list.  \n",
    "    \"\"\"\n",
    "    return unique([record[1][attr] for record in data])\n",
    "\n",
    "def choose_attribute(data, attributes, fitness):\n",
    "    \"\"\"\n",
    "    Cycles through all the attributes and returns the attribute with the\n",
    "    highest information gain (or lowest entropy).\n",
    "    \"\"\"\n",
    "    best_gain = 0.0\n",
    "    best_attr = None\n",
    "\n",
    "    for attr in attributes:\n",
    "        gain = fitness(data, attr)\n",
    "        if gain >= best_gain:\n",
    "            best_gain = gain\n",
    "            best_attr = attr\n",
    "                \n",
    "    return best_attr\n",
    "\n",
    "def get_examples(data, attr, value):\n",
    "    \"\"\"\n",
    "    Returns a list of all the records in <data> with the value of <attr>\n",
    "    matching the given value.\n",
    "    \"\"\"\n",
    "    rtn_lst = []\n",
    "    if not data:\n",
    "        return rtn_lst\n",
    "    else:\n",
    "        for record in data:\n",
    "            if record[1][attr]==value:\n",
    "                rtn_lst.append(record)\n",
    "    return rtn_lst\n",
    "\n",
    "def get_classification(record, tree):\n",
    "    \"\"\"\n",
    "    This function recursively traverses the decision tree and returns a\n",
    "    classification for the given record.\n",
    "    \"\"\"\n",
    "    # If the current node is a string, then we've reached a leaf node and\n",
    "    # we can return it as our answer\n",
    "    if type(tree) == type(\"string\"):\n",
    "        return tree\n",
    "\n",
    "    # Traverse the tree further until a leaf node is found.\n",
    "    else:\n",
    "        attr = tree.keys()[0]\n",
    "        t = tree[attr][record[attr]]\n",
    "        return get_classification(record, t)\n",
    "\n",
    "def classify(tree, data):\n",
    "    \"\"\"\n",
    "    Returns a list of classifications for each of the records in the data\n",
    "    list as determined by the given decision tree.\n",
    "    \"\"\"\n",
    "\n",
    "    classification = []\n",
    "    \n",
    "    for record in data:\n",
    "        classification.append(get_classification(record, tree))\n",
    "\n",
    "    return classification\n",
    "\n",
    "def create_decision_tree(data, attributes, fitness_func):\n",
    "    \"\"\"\n",
    "    Returns a new decision tree based on the examples given.\n",
    "    \"\"\"\n",
    "    vals = [x[0] for x in data]\n",
    "    default = most_frequent(vals)\n",
    "\n",
    "    # If the dataset is empty or the attributes list is empty, return the\n",
    "    # default value. When checking the attributes list for emptiness, we\n",
    "    # need to subtract 1 to account for the target attribute.\n",
    "    if not data or (len(attributes) - 1) <= 0:\n",
    "        return default\n",
    "    # If all the records in the dataset have the same classification,\n",
    "    # return that classification.\n",
    "    elif vals.count(vals[0]) == len(vals):\n",
    "        return vals[0]\n",
    "    else:\n",
    "        # Choose the next best attribute to best classify our data\n",
    "        best = choose_attribute(data, attributes,\n",
    "                                fitness_func)\n",
    "\n",
    "        # Create a new decision tree/node with the best attribute and an empty\n",
    "        # dictionary object--we'll fill that up next.\n",
    "        tree = {best:{}}\n",
    "\n",
    "        # Create a new decision tree/sub-node for each of the values in the\n",
    "        # best attribute field\n",
    "        for val in get_values(data, best):\n",
    "            # Create a subtree for the current value under the \"best\" field\n",
    "            subtree = create_decision_tree(\n",
    "                get_examples(data, best, val),\n",
    "                [attr for attr in attributes if attr != best],\n",
    "                fitness_func)\n",
    "\n",
    "            # Add the new subtree to the empty dictionary object in our new\n",
    "            # tree/node we just created.\n",
    "            tree[best][val] = subtree\n",
    "\n",
    "    return tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def entropy(data):\n",
    "    \"\"\"\n",
    "    Calculates the entropy of the given data set for the target attribute.\n",
    "    \"\"\"\n",
    "    val_freq = {}\n",
    "    data_entropy = 0.0\n",
    "\n",
    "    # Calculate the frequency of each of the values in the target attr\n",
    "    for record in data:\n",
    "        if (val_freq.has_key(record[0])):\n",
    "            val_freq[record[0]] += 1.0\n",
    "        else:\n",
    "            val_freq[record[0]] = 1.0\n",
    "\n",
    "    # Calculate the entropy of the data for the target attribute\n",
    "    for freq in val_freq.values():\n",
    "        data_entropy += (-freq/len(data)) * math.log(freq/len(data), 2) \n",
    "        \n",
    "    return data_entropy\n",
    "    \n",
    "def gain(data, attr):\n",
    "    \"\"\"\n",
    "    Calculates the information gain (reduction in entropy) that would\n",
    "    result by splitting the data on the chosen attribute (attr).\n",
    "    \"\"\"\n",
    "    val_freq = {}\n",
    "    subset_entropy = 0.0\n",
    "\n",
    "    # Calculate the frequency of each of the values in the target attribute\n",
    "    for record in data:\n",
    "        if (val_freq.has_key(record[1][attr])):\n",
    "            val_freq[record[1][attr]] += 1.0\n",
    "        else:\n",
    "            val_freq[record[1][attr]] = 1.0\n",
    "\n",
    "    # Calculate the sum of the entropy for each subset of records weighted\n",
    "    # by their probability of occuring in the training set.\n",
    "    for val in val_freq.keys():\n",
    "        val_prob = val_freq[val] / sum(val_freq.values())\n",
    "        data_subset = [record for record in data if record[1][attr] == val]\n",
    "        subset_entropy += val_prob * entropy(data_subset)\n",
    "\n",
    "    # Subtract the entropy of the chosen attribute from the entropy of the\n",
    "    # whole data set with respect to the target attribute (and return it)\n",
    "    return (entropy(data) - subset_entropy)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support function to take care of continuous variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Loading sample data\n",
    "sample2=[(1,[1,1,30,27,1]),\n",
    "         (0,[0,0,5,8,1]),\n",
    "         (1,[1,1,27,16,2]),\n",
    "         (0,[1,1,2,4,1]),\n",
    "         (0,[0,0,23,8,0]),\n",
    "         (2,[1,0,20,18,1]),\n",
    "         (2,[2,0,15,11,1]),\n",
    "         (2,[1,2,11,25,1]),\n",
    "         (0,[0,1,11,23,1]),\n",
    "         (1,[1,2,8,21,2]),\n",
    "         (0,[1,1,9,9,1]),\n",
    "         (0,[0,1,18,2,0]),\n",
    "         (2,[2,2,7,18,1]),\n",
    "         (2,[2,1,13,6,1]),\n",
    "         (2,[2,2,19,8,1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bisect import bisect\n",
    "def discrete_val(data,ranges):\n",
    "    k = bisect(ranges,float(data))\n",
    "    if k == 0:\n",
    "        return str(ranges[k])+\"<\"\n",
    "    elif k == len(ranges):\n",
    "        return str(ranges[k-1])+\">\"\n",
    "    else:\n",
    "        return str(ranges[k-1])+ \"-\" + str(ranges[k])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Code written according to your input - Not optimized\n",
    "import copy\n",
    "def discretize(data,column_ids,n_bins=10):\n",
    "    new_data = copy.deepcopy(data)\n",
    "    for column in column_ids:\n",
    "        k = [row[1][column] for row in new_data]\n",
    "        col_max = max(k);\n",
    "        col_min = min(k);\n",
    "        ranges = list(np.linspace(col_min, col_max, n_bins))\n",
    "        for idx in range(len(data)):\n",
    "            new_data[idx][1][column] = discrete_val(data[idx][1][column],ranges)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, [1, 1, '30.0>', '27.0>', 1]),\n",
       " (0, [0, 0, '2.0-5.11111111111', '7.55555555556-10.3333333333', 1]),\n",
       " (1, [1, 1, '26.8888888889-30.0', '15.8888888889-18.6666666667', 2]),\n",
       " (0, [1, 1, '2.0-5.11111111111', '2.0-4.77777777778', 1]),\n",
       " (0, [0, 0, '20.6666666667-23.7777777778', '7.55555555556-10.3333333333', 0]),\n",
       " (2, [1, 0, '17.5555555556-20.6666666667', '15.8888888889-18.6666666667', 1]),\n",
       " (2, [2, 0, '14.4444444444-17.5555555556', '10.3333333333-13.1111111111', 1]),\n",
       " (2, [1, 2, '8.22222222222-11.3333333333', '24.2222222222-27.0', 1]),\n",
       " (0, [0, 1, '8.22222222222-11.3333333333', '21.4444444444-24.2222222222', 1]),\n",
       " (1, [1, 2, '5.11111111111-8.22222222222', '18.6666666667-21.4444444444', 2]),\n",
       " (0, [1, 1, '8.22222222222-11.3333333333', '7.55555555556-10.3333333333', 1]),\n",
       " (0, [0, 1, '17.5555555556-20.6666666667', '2.0-4.77777777778', 0]),\n",
       " (2, [2, 2, '5.11111111111-8.22222222222', '15.8888888889-18.6666666667', 1]),\n",
       " (2, [2, 1, '11.3333333333-14.4444444444', '4.77777777778-7.55555555556', 1]),\n",
       " (2, [2, 2, '17.5555555556-20.6666666667', '7.55555555556-10.3333333333', 1])]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discretize(sample2,[2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write a function that will help with bootstrapping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "# method lifted from:http://code.activestate.com/recipes/273085-sample-with-replacement/\n",
    "def sample_wr(population, k):\n",
    "    \"Chooses k random elements (with replacement) from a population\"\n",
    "    n = len(population)\n",
    "    _random, _int = random.random, int  # speed hack \n",
    "    result = [None] * k\n",
    "    for i in xrange(k):\n",
    "        j = _int(_random() * n)\n",
    "        result[i] = population[j]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we create the array of samples to pass to Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# n is number of trees in our forest\n",
    "n=5\n",
    "length=len(sample)\n",
    "forest=[sample]\n",
    "# commence bootstrapping\n",
    "for i in range(1,n):\n",
    "    # add tree to forest\n",
    "    forest.append(sample_wr(sample,length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# m is the number of columns we will be computing on per tree.\n",
    "m=3\n",
    "def choose_columns(num_cols):\n",
    "    return random.sample(range(num_cols),m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we parallelize the forest creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{0: {0: 0, 1: {1: {0: 2, 1: 1, 2: 2}}, 2: 2}},\n",
       " {0: {0: 0, 1: {2: {0: 1, 1: 2, 2: 2}}, 2: 2}},\n",
       " {0: {0: 0, 1: {1: {0: 2, 1: 1, 2: 2}}, 2: 2}},\n",
       " {0: {0: 0, 1: {3: {1: 2, 2: 1}}, 2: 2}},\n",
       " {2: {0: {1: {1: 0, 2: 1}}, 1: {0: {0: 0, 1: 2, 2: 2}}, 2: 2}}]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#parallelize\n",
    "forest_rdd=sc.parallelize(forest)\n",
    "forest_rdd.map(lambda s: create_decision_tree(s,choose_columns(4),gain)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
