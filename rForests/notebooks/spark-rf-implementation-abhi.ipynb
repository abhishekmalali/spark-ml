{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import line_profiler\n",
    "import IPython\n",
    "ip = IPython.get_ipython()\n",
    "ip.define_magic('lprun', line_profiler.magic_lprun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = sc.parallelize([(1,[1,1,0,1]),\n",
    "         (0,[0,0,1,1]),\n",
    "         (1,[1,1,0,2]),\n",
    "         (0,[1,1,0,1]),\n",
    "         (0,[0,0,1,0]),\n",
    "         (2,[1,0,2,1]),\n",
    "         (2,[2,0,1,1]),\n",
    "         (2,[1,2,1,1]),\n",
    "         (0,[0,1,1,1]),\n",
    "         (1,[1,2,0,2]),\n",
    "         (0,[1,1,1,1]),\n",
    "         (0,[0,1,1,0]),\n",
    "         (2,[2,2,2,1]),\n",
    "         (2,[2,1,1,1]),\n",
    "         (2,[2,2,1,1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample2 = sc.textFile('../data/balance-scale.csv')\n",
    "rdd2=sample2.map(lambda x:x.split()).map(lambda x: x[0].strip(\"'\").split(\",\"))\\\n",
    "            .map(lambda x:[v for v in x])\\\n",
    "            .map(lambda x: (str(x[0]),[int(k) for k in x[1:]]))\n",
    "columns2 = ['Left-Weight','Left-Distance','Right-Weight','Right-Distance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('B', [1, 1, 1, 1]), ('R', [1, 1, 1, 2])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "txtFile=sc.textFile('../data/covtype.csv')\n",
    "#Convert it into RDD of lists \n",
    "rdd=(txtFile.map(lambda x:x.split())\n",
    "    .map(lambda x: x[0].strip(\"'\").split(\",\"))\n",
    "    .map(lambda x:[float(v) for v in x])\n",
    "    .map(lambda x: (x[-1]-1,x[0:-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "soil_list =[]\n",
    "for k in range(40):\n",
    "    string = 'Soil_Type_' + str(k+1)\n",
    "    soil_list.append(string)\n",
    "WA_list =[]\n",
    "for k in range(4):\n",
    "    string = 'WA_' + str(k+1)\n",
    "    WA_list.append(string)\n",
    "names = [['Elevation'], ['Aspect'], ['Slope'], ['HDHyrdo'], ['VDHydro'], ['HDRoadways'], \\\n",
    "         ['9amHills'],['NoonHills'], ['3pmHills'], ['HDFirePoints'], WA_list,\\\n",
    "         soil_list, ['Cover_Type']]\n",
    "columns = list(itertools.chain(*names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 26, 47, 44, 38]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "#selecting random features\n",
    "m = 5 #No of features\n",
    "indices = range(len(columns)-1)\n",
    "random.shuffle(indices)\n",
    "print indices[:m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampled_rdd = rdd.map(lambda x: (str(x[0]),[x[1][j] for j in indices[:m]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampled_rdd = discretize_columns(sampled_rdd,[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col_sampled = [columns[i] for i in indices[:m]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### InfoGain function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "def IG(l):\n",
    "    length=len(l)\n",
    "    c=Counter()\n",
    "    for v in l:\n",
    "        c[v] += 1.0/length\n",
    "    return 1-sum(np.multiply(c.values(),c.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def infoGain2(sampled_rdd,count,attr=0):\n",
    "    sampled_rdd.cache()\n",
    "    output = sampled_rdd.map(lambda x: (x[1][attr],x[0]))\\\n",
    "            .groupByKey().mapValues(lambda x: tuple(x))\\\n",
    "            .map(lambda x: (x[0],x[1], len(x[1])/float(count)))\\\n",
    "            .map(lambda x: (x[0],IG(x[1]),x[2]))\\\n",
    "            .map(lambda x: x[1]*x[2]).reduce(lambda a,b:a+b)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def choose_attribute(sampled_rdd,attributes, count):\n",
    "    best_gain = float(\"inf\")\n",
    "    best_attr = None\n",
    "    for att in attributes:\n",
    "        gain = infoGain2(sampled_rdd,count,att)\n",
    "        #print att,gain\n",
    "        if gain <= best_gain:\n",
    "            best_gain = gain\n",
    "            best_attr = att\n",
    "    cats=sampled_rdd.map(lambda x:x[1][best_attr]).distinct().collect()\n",
    "    return best_attr,cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function returns most frequent value in response variable\n",
    "def most_frequent(data):\n",
    "    highest_freq = 0\n",
    "    most_freq = None\n",
    "    vals = data.map(lambda x:x[0]).distinct().collect()\n",
    "    for val in vals:\n",
    "        freq = data.filter(lambda x:x[0] == val)\\\n",
    "                    .map(lambda x:x[0]).count()\n",
    "        if freq > highest_freq:\n",
    "            most_freq = val\n",
    "            highest_freq = freq\n",
    "    return most_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "def createDecisionTree(sub_rdd,attributes,columns):\n",
    "    if len(attributes) <= 0:\n",
    "        return most_frequent(sub_rdd)\n",
    "    elif sub_rdd.map(lambda x:x[0]).distinct().count() == 1:\n",
    "        return sub_rdd.map(lambda x:x[0]).distinct().collect()[0][0]\n",
    "    ##changes\n",
    "    elif sub_rdd.count() == 0:\n",
    "        return 0\n",
    "    ##changes\n",
    "    else:\n",
    "        bestAttr,vals = choose_attribute(sub_rdd,attributes,sub_rdd.count())\n",
    "        attributes.remove(bestAttr)\n",
    "        #print bestAttr,vals\n",
    "        tree = {columns[bestAttr]:{}}\n",
    "        for val in vals:\n",
    "            new_rdd = sub_rdd.filter(lambda x:x[1][bestAttr] == val)\\\n",
    "            .map(lambda x:(x[0],x[1]))\n",
    "            #print val,bestAttr,attributes\n",
    "            new_attributes = copy.deepcopy(attributes)\n",
    "            subtree = createDecisionTree(new_rdd,new_attributes,columns)\n",
    "            tree[columns[bestAttr]][val] = subtree\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%lprun -f IG tree = createDecisionTree(rdd2,range(len(columns2)),columns2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.2 s, sys: 4.04 s, total: 21.2 s\n",
      "Wall time: 1min 45s\n"
     ]
    }
   ],
   "source": [
    "%time tree = createDecisionTree(rdd2,range(len(columns2)),columns2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function for testing a tree on a set of data\n",
    "def tree_test(data,columns,tree):\n",
    "    #data will be the list stored in the second half of the tuple\n",
    "    #columns contains the name of columns for referencing the tree\n",
    "    if type(tree) == type(\"string\"):\n",
    "        return tree\n",
    "    else:\n",
    "        attr = tree.keys()[0]\n",
    "        t = tree[attr][data[columns.index(attr)]]\n",
    "        return tree_test(data,columns,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Applying tree test row wise to the RDD\n",
    "res_rdd = rdd2.map(lambda x:(x[0],tree_test(x[1],columns2,tree)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.0\n"
     ]
    }
   ],
   "source": [
    "#Calculating accuracy\n",
    "print \"Accuracy:\", 100*(res_rdd.filter(lambda x:x[0] == x[1]).count())/float(res_rdd.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding support for continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Loading dataset with continuous features\n",
    "sample_c = sc.textFile('../data/crx.csv')\n",
    "rdd_c=sample_c.map(lambda x:x.split()).map(lambda x: x[0].strip(\"'\").split(\",\"))\\\n",
    "            .map(lambda x:[v for v in x])\\\n",
    "            .map(lambda x: (str(x[-1]),[k for k in x[0:-1]]))\n",
    "columns_c = []\n",
    "for k in range(1,16):\n",
    "    columns_c.append('A'+str(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bisect import bisect\n",
    "def discrete_val(data,ranges):\n",
    "    k = bisect(ranges,float(data))\n",
    "    print k\n",
    "    if k == 0:\n",
    "        return str(ranges[k])+\"<\"\n",
    "    elif k == len(ranges):\n",
    "        return str(ranges[k-1])+\">\"\n",
    "    else:\n",
    "        return str(ranges[k-1])+ \"-\" + str(ranges[k])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Provide list of column numbers for columns to be discretized\n",
    "import operator\n",
    "def discretize_column(data,column,n_bins):\n",
    "    col_max = data.map(lambda x:float(x[1][column])).max()\n",
    "    col_min = data.map(lambda x:float(x[1][column])).min()\n",
    "    ranges = list(np.linspace(col_min, col_max, n_bins))\n",
    "    new_data = data.map(lambda x:(x[0],[x[1][0:column],[discrete_val(x[1][column],ranges)],x[1][column+1:]]))\n",
    "    new_data = new_data.map(lambda x:(x[0],reduce(operator.add, x[1])))\n",
    "    data = new_data\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discretize_columns(data,column_ids,n_bins=10):\n",
    "    col_count = 0\n",
    "    for column in column_ids:\n",
    "        if col_count == 0:\n",
    "            return_rdd = discretize_column(data,column,n_bins)\n",
    "        else:\n",
    "            return_rdd = discretize_column(return_rdd,column,n_bins)\n",
    "        col_count += 1\n",
    "    return return_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_rdd = discretize_columns(rdd_c,[1,2,7,10,13,14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'7-8'"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discrete_val(7.5,[1,2,3,4,5,6,7,8,9,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "res_rdd_c = test_rdd.map(lambda x:(x[0],tree_test(x[1],columns_c,test_tree)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.5405819296\n"
     ]
    }
   ],
   "source": [
    "#Calculating accuracy\n",
    "print \"Accuracy:\", 100*(res_rdd_c.filter(lambda x:x[0] == x[1]).count())/float(res_rdd_c.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For Main dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('4.0', ['225.777777778-254.0', 0.0, 0.0, 0.0, 0.0]),\n",
       " ('4.0', ['225.777777778-254.0', 0.0, 0.0, 0.0, 0.0])]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time tree_test = createDecisionTree(sampled_rdd,range(len(col_sampled)),col_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
