{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import scipy as sc\n",
    "import autograd.scipy as sci  # Thinly-wrapped scipy\n",
    "import autograd.numpy as np  # Thinly-wrapped numpy\n",
    "from autograd import grad\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import os\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x1047e48d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load joke data including features\n",
    "\n",
    "# load data using pandas\n",
    "df = pd.read_csv('../data/ratings.csv', sep=' ', header=None)\n",
    "# uid = user id\n",
    "# jid = joke id\n",
    "df.columns = ['uid', 'jid', 'rating']\n",
    "\n",
    "\n",
    "# Let N be the number of users, M be the number of jokes\n",
    "# use +1 for easier referencing later\n",
    "I = df.uid.max() + 1\n",
    "J = df.jid.max() + 1\n",
    "\n",
    "# for this problem, we do not need uid --> remove column!\n",
    "#df = df[['rating', 'jid']]\n",
    "\n",
    "# import joke features Jf\n",
    "dfJ = pd.read_csv('../data/features.csv', sep=' ', header=None)\n",
    "wrange = ['j'+str(w) for w in range(151)]\n",
    "dfJ.columns = wrange\n",
    "dfJ.head()\n",
    "\n",
    "# convert to numpy matrix Jf\n",
    "Jf = dfJ.values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loglikelihood is given as\n",
    "$$ \\mathcal{L}(\\beta \\; \\vert \\; \\mathcal{D}, \\theta) = \\sum_{i=1}^n \\log \\left( F_\\epsilon(\\theta_{Y_i} + X_i^T \\beta) - F_\\epsilon(\\theta_{Y_i-1} + X_i^T\\beta) \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the log-likelihood function\n",
    "# Xrat array consisting of (rating, user, item)\n",
    "# Xfeat array indexable by the item column of Xrat which delivers item features\n",
    "# theta is the parameter vector\n",
    "# theta = (u_1, ..., u_I, v_1, ..., v_J, a_1, ..., a_I, b_1, ..., b_J, g, beta_1, ..., beta_L)\n",
    "# i.e. theta has length (I + J) * K + I + J + 1 + L\n",
    "# with u_i, v_j \\in \\mathbb{R}^K\n",
    "# b stores the buckets for R categories (i.e. we have R-1 elements in b)\n",
    "# i.e. in b we store b_1, b_2, ..., b_{R-1} for R different categories\n",
    "def loglikelihood(theta, Xrat, Xfeat, b, I, J, K, R):\n",
    "    \n",
    "    L = Xfeat.shape[1]\n",
    "    \n",
    "    # first implement the easy latent model using probit...\n",
    "    llsum = 0\n",
    "    for row in xrange(Xrat.shape[0]):\n",
    "        rating = Xrat[row, 0]\n",
    "        i = Xrat[row, 1]\n",
    "        j = Xrat[row, 2]\n",
    "\n",
    "        # asserts for the indices i, j & rating\n",
    "        assert i < I and i >= 0\n",
    "        assert j < J and j >= 0\n",
    "        assert Xfeat.shape[0] == J\n",
    "        assert rating > 0 and rating <= R\n",
    "\n",
    "        # the model for the latent variable\n",
    "        u_i = theta[K * i:K*(i+1)]\n",
    "        v_j = theta[(I + j) * K:(I + j + 1) * K]\n",
    "        a_i = theta[(I + J) * K + i]\n",
    "        b_j = theta[(I + J) * K + I + j]\n",
    "        g = theta[(I + J) * K + I + J]\n",
    "        beta = theta[(I + J) * K + I + J + 1:]\n",
    "\n",
    "        # some asserts for the sizes\n",
    "        assert len(u_i) == K\n",
    "        assert len(v_j) == K\n",
    "        assert len(beta) == L\n",
    "\n",
    "        # the full model (reduce if necessary)\n",
    "        # model with features does not work yet...\n",
    "        model = np.dot(u_i, v_j) + a_i + b_j + g + np.dot(Xfeat[j, :], beta) \n",
    "\n",
    "        # here are some other choices\n",
    "\n",
    "        # easy model using features only\n",
    "        # model = np.dot(Xfeat[j, :], beta)\n",
    "\n",
    "        # model using features + biases\n",
    "        # model = np.dot(Xfeat[j, :], beta) + a_i + b_j + g\n",
    "\n",
    "        # model using latent factors for user/item\n",
    "        # model = np.dot(u_i, v_j)\n",
    "\n",
    "        # model using latent factors for user/item and biases\n",
    "        # model = np.dot(u_i, v_j) + a_i + b_j + g\n",
    "\n",
    "        # the ordinal regression part\n",
    "        # if rating is 1 or R we have a special case\n",
    "        # another possibility would be to use instead of the ifelse construction\n",
    "        # dummy values like +/- 9999 for infty\n",
    "        # note the additional -1 due to space saving!\n",
    "        if rating == R:\n",
    "            # note F(infty) = 0 (mathematically not rigourous, limit is more correct)\n",
    "            llsum += np.log(sci.stats.norm.cdf(b[rating - 2] + model))\n",
    "        elif rating == 1:\n",
    "            # note F(-infty) = 0\n",
    "            llsum += np.log(1 - sci.stats.norm.cdf(b[rating - 1] + model))\n",
    "        else:\n",
    "            llsum += np.log(sci.stats.norm.cdf(b[rating - 1] + model) - sci.stats.norm.cdf(b[rating - 2] + model))\n",
    "    return llsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8947.9186545983339"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing the gradient\n",
    "\n",
    "# create sample df\n",
    "dftouse = df[['rating', 'uid', 'jid']].head(2000)\n",
    "\n",
    "# as in the given dataset the indices are 1,...,I and 1, ..., J\n",
    "# adjust jokes & uid s.t. they serve the 0, ..., I-1 and 0, ..., J-1 space \n",
    "dftouse['uid'] = dftouse['uid'] - 1\n",
    "dftouse['jid'] = dftouse['jid'] - 1\n",
    "\n",
    "# transform ratings to range 1, ..., R\n",
    "rating_vals = np.sort(pd.unique(dftouse['rating'].values.ravel()))\n",
    "minR = dftouse.rating.min()\n",
    "dftouse['rating'] = dftouse['rating'] - minR + 1\n",
    "R = len(rating_vals)\n",
    "\n",
    "# create buckets as midpoints\n",
    "buckets = 0.5 * (rating_vals[1:] + rating_vals[:-1])\n",
    "\n",
    "# get length I, J\n",
    "I = dftouse.uid.max() + 1\n",
    "J = dftouse.jid.max() + 1\n",
    "\n",
    "# define some K\n",
    "K = 2\n",
    "\n",
    "# convert to numpy data matrix\n",
    "Xrat = np.array(dftouse)\n",
    "Xfeat = Jf\n",
    "\n",
    "# create dummy theta vector (all zeros)\n",
    "L = Xfeat.shape[1]\n",
    "theta = np.zeros((I + J) * K + I + J + 1 + L)\n",
    "\n",
    "# init theta vector with some random values\n",
    "theta = np.random.normal(size=theta.shape[0], loc=0., scale=0.1)\n",
    "\n",
    "# compute log likelihood\n",
    "loglikelihood(theta, Xrat, Xfeat, buckets, I, J, K, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## the rowlikelihood for sgd\n",
    "def rowloglikelihood(theta, row, Xfeat, b, I, J, K, R):\n",
    "    L = Xfeat.shape[1]\n",
    "    rating = row[0]\n",
    "    i = row[1]\n",
    "    j = row[2]\n",
    "\n",
    "    # asserts for the indices i, j & rating\n",
    "    assert i < I and i >= 0\n",
    "    assert j < J and j >= 0\n",
    "    assert Xfeat.shape[0] == J\n",
    "    assert rating > 0 and rating <= R\n",
    "\n",
    "    # the model for the latent variable\n",
    "    u_i = theta[K * i:K*(i+1)]\n",
    "    v_j = theta[(I + j) * K:(I + j + 1) * K]\n",
    "    a_i = theta[(I + J) * K + i]\n",
    "    b_j = theta[(I + J) * K + I + j]\n",
    "    g = theta[(I + J) * K + I + J]\n",
    "    beta = theta[(I + J) * K + I + J + 1:]\n",
    "\n",
    "    # some asserts for the sizes\n",
    "    assert len(u_i) == K\n",
    "    assert len(v_j) == K\n",
    "    assert len(beta) == L\n",
    "\n",
    "    # the full model (reduce if necessary)\n",
    "    # model with features does not work yet...\n",
    "    model = np.dot(u_i, v_j) + a_i + b_j + g + np.dot(Xfeat[j, :], beta) \n",
    "    \n",
    "    # here are some other choices\n",
    "    \n",
    "    # easy model using features only\n",
    "    # model = np.dot(Xfeat[j, :], beta)\n",
    "    \n",
    "    # model using features + biases\n",
    "    # model = np.dot(Xfeat[j, :], beta) + a_i + b_j + g\n",
    "    \n",
    "    # model using latent factors for user/item\n",
    "    # model = np.dot(u_i, v_j)\n",
    "    \n",
    "    # model using latent factors for user/item and biases\n",
    "    # model = np.dot(u_i, v_j) + a_i + b_j + g\n",
    "\n",
    "    # the ordinal regression part\n",
    "    # if rating is 1 or R we have a special case\n",
    "    # another possibility would be to use instead of the ifelse construction\n",
    "    # dummy values like +/- 9999 for infty\n",
    "    # note the additional -1 due to space saving!\n",
    "    if rating == R:\n",
    "        # note F(infty) = 0 (mathematically not rigourous, limit is more correct)\n",
    "        return np.log(sci.stats.norm.cdf(b[rating - 2] + model))\n",
    "    elif rating == 1:\n",
    "        # note F(-infty) = 0\n",
    "        return np.log(1 - sci.stats.norm.cdf(b[rating - 1] + model))\n",
    "    else:\n",
    "        return np.log(sci.stats.norm.cdf(b[rating - 1] + model) - sci.stats.norm.cdf(b[rating - 2] + model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        , ..., -3.65815073,\n",
       "        0.        , -3.65815073])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use autograd to get a gradient out of the rowlikelihood\n",
    "gradrll = grad(rowloglikelihood)\n",
    "gradll = grad(loglikelihood)\n",
    "\n",
    "# check that gradient has the right length and output sample\n",
    "assert len(gradrll(theta, Xrat[5], Xfeat, buckets, I, J, K, R)) == (I + J) * K + I + J + 1 + L\n",
    "gradrll(theta, Xrat[5], Xfeat, buckets, I, J, K, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-8065.0295793309624, -8065.0295793309624)\n",
      "[ -9.38462676e-03  -1.12376626e-02   0.00000000e+00 ...,  -2.50382169e+02\n",
      "  -3.44893040e+02  -4.16101994e+03]\n",
      "[ -9.38462676e-03  -1.12376626e-02   0.00000000e+00 ...,  -2.50382169e+02\n",
      "  -3.44893040e+02  -4.16101994e+03]\n"
     ]
    }
   ],
   "source": [
    "# test the row likelihood by comparing to the full one\n",
    "llsum = 0.\n",
    "for row in Xrat:\n",
    "    llsum += rowloglikelihood(theta, row, Xfeat, buckets, I, J, K, R)\n",
    "print (llsum, loglikelihood(theta, Xrat, Xfeat, buckets, I, J, K, R))\n",
    "\n",
    "# do analogously the gradient test...\n",
    "gsum = 0.\n",
    "for row in Xrat:\n",
    "    gsum += gradrll(theta, row, Xfeat, buckets, I, J, K, R)\n",
    "print gsum\n",
    "print gradll(theta, Xrat, Xfeat, buckets, I, J, K, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0.             0.             0.         ...,  -350.56791086\n",
      "  -425.33674269 -4612.06896786]\n"
     ]
    }
   ],
   "source": [
    "# do sgd approximation\n",
    "M = 100 # choose M samples\n",
    "gsum = 0.\n",
    "for row in np.random.randint(Xrat.shape[0], size=M):\n",
    "    # don't forget scaling!\n",
    "    gsum += 1. * Xrat.shape[0] / M * gradrll(theta, Xrat[row], Xfeat, buckets, I, J, K, R)\n",
    "print gsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0050993   0.06732006  0.08307213 ..., -0.01384602 -0.11538214\n",
      " -1.40406265]\n",
      "Time taken: 7.128969 s\n",
      "-1850.51150326\n"
     ]
    }
   ],
   "source": [
    "# one epoch of sgd!\n",
    "\n",
    "theta0 = theta.copy()\n",
    "\n",
    "# combined learning_rate * scale_factor\n",
    "alpha = 0.05\n",
    "# shuffle data here!\n",
    "\n",
    "t=time.time()\n",
    "for row in Xrat:\n",
    "    # update theta0 according to current row\n",
    "    theta0 += alpha * gradrll(theta0, row, Xfeat, buckets, I, J, K, R)\n",
    "print theta0\n",
    "\n",
    "print \"Time taken: %f s\" % (time.time()-t)\n",
    "print loglikelihood(theta0, Xrat, Xfeat, buckets, I, J, K, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0050993 ,  0.06732006,  0.08307213, ..., -0.01384602,\n",
       "       -0.11538214, -1.40406265])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 66 ms, sys: 97 ms, total: 163 ms\n",
      "Wall time: 9.55 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-4875.4270370881022"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now to parallelize! Stick Xrat into an RDD\n",
    "xrat_rdd=sc.parallelize(Xrat)\n",
    "\n",
    "# Get a fresh theta\n",
    "theta1=theta.copy()\n",
    "\n",
    "#And then compute the gradient for each row individually, and the sum it\n",
    "%time ptheta1=xrat_rdd.map(lambda x: alpha*gradrll(theta1,x,Xfeat, buckets, I, J, K, R)).mean()\n",
    "n=xrat_rdd.count()\n",
    "# subtract out (n-1)*theta1 to get final theta\n",
    "loglikelihood(ptheta1, Xrat, Xfeat, buckets, I, J, K, R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That log likelihood is pretty awful though. Lets break it up into subarrays of size 5 and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A function we pass to the array to calculate the updated thetas from each subarray\n",
    "\n",
    "def n_row_sgd(theta,subX, Xfeat, buckets, I, J, K, R):\n",
    "    for row in subX:\n",
    "        # update theta0 according to current row\n",
    "        theta += alpha * gradrll(theta, row, Xfeat, buckets, I, J, K, R)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 75.3 ms, sys: 108 ms, total: 183 ms\n",
      "Wall time: 6.68 s\n"
     ]
    }
   ],
   "source": [
    "size=5\n",
    "#Split the array into subarrays of size n\n",
    "split_xrat=np.split(Xrat,Xrat.shape[0]/size)\n",
    "\n",
    "#And then parallelize it\n",
    "split_xrat = sc.parallelize(split_xrat,5)\n",
    "\n",
    "# Get a fresh theta\n",
    "theta2=theta.copy()\n",
    "\n",
    "# Run the sgd!\n",
    "%time ptheta2=split_xrat.map(lambda subX:n_row_sgd(theta2, subX, Xfeat, buckets, I, J, K, R)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1640.866923790534"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loglikelihood(ptheta2, Xrat, Xfeat, buckets, I, J, K, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00481152,  0.06098107,  0.08307213, ..., -0.13811418,\n",
       "       -0.09687869, -0.1490642 ])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Overall Todos:\n",
    "# # add Documentation, code in MLib style\n",
    "\n",
    "# # This shall be designed for modelling ratings\n",
    "# # assume we are given n users i =1, ..., I\n",
    "# # that rated m items j= 1, ..., J\n",
    "# # Y_n ~ u_i^Tv_j + a_i + b_j + g + X_n * w\n",
    "# # u_i, v_j are K-dimensional latent variable vectors\n",
    "# # a_i, b_j, g are user/item/global wise biases (latent)\n",
    "# # X_n represents the n-th data row with L features\n",
    "# # w is the weight vector we want to train for\n",
    "# # Thus, in total our model trains the parameter vector\n",
    "# # theta = (u_1, ..., u_I, v_1, ..., v_J, a_1, ..., a_I, b_1, ..., b_J, g, w)\n",
    "# class CumulativeModel:\n",
    "    \n",
    "    \n",
    "#     def __init__(self, buckets = None, modelType = 'logistic', spark=False, numLatentFactors=0):\n",
    "#         # add here check for correct model types\n",
    "#         # logistic, probit, minev, maxev\n",
    "#         self.mtype = modelType\n",
    "#         self.useSpark = spark\n",
    "#         self.K = numLatentFactors\n",
    "#         self.buckets = buckets\n",
    "        \n",
    "#     # add here sgd parameters\n",
    "#     def fit(self, X, y, ...):\n",
    "#         # first set buckets if necessary\n",
    "#         if self.buckets is None:\n",
    "#             warning()\n",
    "        \n",
    "#     # make a prediction\n",
    "#     def predict(self, X, y, ...):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
