{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Ordinal Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Imports\n",
    "import matplotlib.pyplot as plt\n",
    "# import scipy as sco\n",
    "import autograd.scipy as sci  # Thinly-wrapped scipy\n",
    "import autograd.numpy as np  # Thinly-wrapped numpy\n",
    "from autograd import grad\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import os\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "sc=pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('../data/musicdata.csv',header=None)\n",
    "df.columns=['uid', 'aid', 'rating']\n",
    "\n",
    "# I and J are the number of users and artists, respectively\n",
    "I = df.uid.max() + 1\n",
    "J = df.aid.max() + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loglikelihood is given as\n",
    "$$ \\mathcal{L}(\\beta \\; \\vert \\; \\mathcal{D}, \\theta) = \\sum_{i=1}^n \\log \\left( F_\\epsilon(\\theta_{Y_i} + X_i^T \\beta) - F_\\epsilon(\\theta_{Y_i-1} + X_i^T\\beta) \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first write a log likelihood function that calculates it given an entire array, and iterates over the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the log-likelihood function\n",
    "# Xrat array consisting of (rating, user, item)\n",
    "# Xfeat array indexable by the item column of Xrat which delivers item features\n",
    "# theta is the parameter vector\n",
    "# theta = (u_1, ..., u_I, v_1, ..., v_J, a_1, ..., a_I, b_1, ..., b_J, g, beta_1, ..., beta_L)\n",
    "# i.e. theta has length (I + J) * K + I + J + 1 + L\n",
    "# with u_i, v_j \\in \\mathbb{R}^K\n",
    "# b stores the buckets for R categories (i.e. we have R-1 elements in b)\n",
    "# i.e. in b we store b_1, b_2, ..., b_{R-1} for R different categories\n",
    "def loglikelihood(theta, Xrat, b, I, J, K, R):\n",
    "    \n",
    "    \n",
    "    # first implement the easy latent model using probit...\n",
    "    llsum = 0\n",
    "    for row in xrange(Xrat.shape[0]):\n",
    "        rating = Xrat[row, 0]\n",
    "        i = Xrat[row, 1]\n",
    "        j = Xrat[row, 2]\n",
    "\n",
    "        # asserts for the indices i, j & rating\n",
    "        assert i < I and i >= 0\n",
    "        assert j < J and j >= 0\n",
    "        assert rating <= R\n",
    "\n",
    "        # the model for the latent variable\n",
    "        u_i = theta[K * i:K*(i+1)]\n",
    "        v_j = theta[(I + j) * K:(I + j + 1) * K]\n",
    "        a_i = theta[(I + J) * K + i]\n",
    "        b_j = theta[(I + J) * K + I + j]\n",
    "        g = theta[(I + J) * K + I + J]\n",
    "        beta = theta[(I + J) * K + I + J + 1:]\n",
    "\n",
    "        # some asserts for the sizes\n",
    "        assert len(u_i) == K\n",
    "        assert len(v_j) == K\n",
    "\n",
    "        # the full model (reduce if necessary)\n",
    "        # model with features does not work yet...\n",
    "#         model = np.dot(u_i, v_j) + a_i + b_j + g + np.dot(Xfeat[j, :], beta) \n",
    "\n",
    "        # here are some other choices\n",
    "\n",
    "        # easy model using features only\n",
    "        # model = np.dot(Xfeat[j, :], beta)\n",
    "\n",
    "        # model using features + biases\n",
    "        # model = np.dot(Xfeat[j, :], beta) + a_i + b_j + g\n",
    "\n",
    "        # model using latent factors for user/item\n",
    "#         model = np.dot(u_i, v_j)\n",
    "\n",
    "        # model using latent factors for user/item and biases\n",
    "        model = np.dot(u_i, v_j) + a_i + b_j + g\n",
    "\n",
    "        # the ordinal regression part\n",
    "        # if rating is 1 or R we have a special case\n",
    "        # another possibility would be to use instead of the ifelse construction\n",
    "        # dummy values like +/- 9999 for infty\n",
    "        # note the additional -1 due to space saving!\n",
    "        if rating == R:\n",
    "            # note F(infty) = 0 (mathematically not rigourous, limit is more correct)\n",
    "            llsum += np.log(1 - sci.stats.norm.cdf(b[rating - 2] + model))\n",
    "        elif rating == 1:\n",
    "            # note F(-infty) = 0\n",
    "            llsum += np.log(sci.stats.norm.cdf(b[rating - 1] + model))\n",
    "        else:\n",
    "            llsum += np.log(sci.stats.norm.cdf(b[rating - 1] + model) - sci.stats.norm.cdf(b[rating - 2] + model))\n",
    "    return llsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define a gradient function, in the same manner, that calculates the gradient row by row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# by hand derived gradient\n",
    "def manual_grad(theta, Xrat, b, I, J, K, R):\n",
    "    grad = np.zeros(theta.shape[0])\n",
    "    \n",
    "    # go over samples and make update according to them!\n",
    "    for n in xrange(Xrat.shape[0]):\n",
    "        \n",
    "        rating = Xrat[n, 0]\n",
    "        i = Xrat[n, 1]\n",
    "        j = Xrat[n, 2]\n",
    "        \n",
    "        # the model for the latent variable\n",
    "        u_i = theta[K * i:K*(i+1)]\n",
    "        v_j = theta[(I + j) * K:(I + j + 1) * K]\n",
    "        a_i = theta[(I + J) * K + i]\n",
    "        b_j = theta[(I + J) * K + I + j]\n",
    "        g = theta[(I + J) * K + I + J]\n",
    "        beta = theta[(I + J) * K + I + J + 1:]\n",
    "\n",
    "        # some asserts for the sizes\n",
    "        assert len(u_i) == K\n",
    "        assert len(v_j) == K\n",
    "        \n",
    "        model = np.dot(u_i, v_j)\n",
    "        if rating == R:\n",
    "            q = 1 - sc.stats.norm.pdf(b[rating - 2] + model) / (1 - sc.stats.norm.cdf(b[rating - 2] + model))\n",
    "        elif rating == 1:\n",
    "            q = sc.stats.norm.pdf(b[rating - 1] + model) / sc.stats.norm.cdf(b[rating - 1] + model)\n",
    "        else:\n",
    "            q = sc.stats.norm.pdf(b[rating-2] + model) - sc.stats.norm.pdf(b[rating - 1] + model) / (sc.stats.norm.cdf(b[rating-2] + model) - sc.stats.norm.cdf(b[rating - 1] + model))\n",
    "        \n",
    "        # compute derivatives\n",
    "        grad_u_i = q * v_j\n",
    "        grad_v_j = q * u_i\n",
    "        grad_a_i = q \n",
    "        grad_b_j = q\n",
    "        grad_g = q\n",
    "#         grad_beta = q * Xfeat[j, :]\n",
    "        \n",
    "        grad[K * i:K*(i+1)] += grad_u_i\n",
    "        grad[(I + j) * K:(I + j + 1) * K] += grad_v_j\n",
    "        grad[(I + J) * K + i] += grad_a_i\n",
    "        grad[(I + J) * K + I + j] += grad_b_j\n",
    "        grad[(I + J) * K + I + J] += grad_g\n",
    "#         grad[(I + J) * K + I + J + 1:] += grad_beta\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is then prepped. We begin with a random theta vector, and calculate the log likelihood to ensure that it is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-16018.296162212204"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# create sample df\n",
    "dftouse = df[['rating', 'uid', 'aid']].head(2000)\n",
    "dftouse.rating=np.around((dftouse.rating-1)/20)\n",
    "# as in the given dataset the indices are 1,...,I and 1, ..., J\n",
    "# adjust jokes & uid s.t. they serve the 0, ..., I-1 and 0, ..., J-1 space \n",
    "dftouse['uid'] = dftouse['uid'] - 1\n",
    "dftouse['aid'] = dftouse['aid'] - 1\n",
    "\n",
    "# transform ratings to range 1, ..., R\n",
    "rating_vals = np.arange(1,dftouse.rating.max()+1)\n",
    "minR = dftouse.rating.min()\n",
    "dftouse['rating'] = dftouse['rating'] - minR\n",
    "R = len(rating_vals)\n",
    "\n",
    "\n",
    "# create buckets as midpoints\n",
    "buckets = 0.5 * (rating_vals[1:] + rating_vals[:-1])\n",
    "\n",
    "# get length I, J\n",
    "I = dftouse.uid.max() + 1\n",
    "J = dftouse.aid.max() + 1\n",
    "\n",
    "# define some K\n",
    "K = 2\n",
    "\n",
    "# convert to numpy data matrix\n",
    "Xrat = np.array(dftouse)\n",
    "# Xfeat = Jf\n",
    "\n",
    "# create dummy theta vector (all zeros)\n",
    "# L = Xfeat.shape[1]\n",
    "theta = np.zeros((I + J) * K + I + J + 1)\n",
    "\n",
    "# init theta vector with some random values\n",
    "theta = np.random.normal(size=theta.shape[0], loc=0., scale=0.1)\n",
    "\n",
    "# compute log likelihood\n",
    "loglikelihood(theta, Xrat, buckets, I, J, K, R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function calculates the likelihood like before, but this time only for a single row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## the rowlikelihood for sgd\n",
    "def rowloglikelihood(theta, row, b, I, J, K, R):\n",
    "    rating = row[0]\n",
    "    i = row[1]\n",
    "    j = row[2]\n",
    "\n",
    "    # asserts for the indices i, j & rating\n",
    "    assert i < I and i >= 0\n",
    "    assert j < J and j >= 0\n",
    "    assert rating <= R\n",
    "\n",
    "    # the model for the latent variable\n",
    "    u_i = theta[K * i:K*(i+1)]\n",
    "    v_j = theta[(I + j) * K:(I + j + 1) * K]\n",
    "    a_i = theta[(I + J) * K + i]\n",
    "    b_j = theta[(I + J) * K + I + j]\n",
    "    g = theta[(I + J) * K + I + J]\n",
    "    beta = theta[(I + J) * K + I + J + 1:]\n",
    "\n",
    "    # some asserts for the sizes\n",
    "    assert len(u_i) == K\n",
    "    assert len(v_j) == K\n",
    "\n",
    "    # the full model (reduce if necessary)\n",
    "    # model with features does not work yet...\n",
    "#     model = np.dot(u_i, v_j) + a_i + b_j + g + np.dot(Xfeat[j, :], beta) \n",
    "    \n",
    "    # here are some other choices\n",
    "    \n",
    "    # easy model using features only\n",
    "    # model = np.dot(Xfeat[j, :], beta)\n",
    "    \n",
    "    # model using features + biases\n",
    "    # model = np.dot(Xfeat[j, :], beta) + a_i + b_j + g\n",
    "    \n",
    "    # model using latent factors for user/item\n",
    "#     model = np.dot(u_i, v_j)\n",
    "    \n",
    "    # model using latent factors for user/item and biases\n",
    "    model = np.dot(u_i, v_j) + a_i + b_j + g\n",
    "\n",
    "    # the ordinal regression part\n",
    "    # if rating is 1 or R we have a special case\n",
    "    # another possibility would be to use instead of the ifelse construction\n",
    "    # dummy values like +/- 9999 for infty\n",
    "    # note the additional -1 due to space saving!\n",
    "    if rating == R:\n",
    "        # note F(infty) = 0 (mathematically not rigourous, limit is more correct)\n",
    "        return np.log(1 - sci.stats.norm.cdf(b[rating - 2] + model))\n",
    "    elif rating == 1:\n",
    "        # note F(-infty) = 0\n",
    "        return np.log(sci.stats.norm.cdf(b[rating - 1] + model))\n",
    "    else:\n",
    "        return np.log(sci.stats.norm.cdf(b[rating - 1] + model) - \\\n",
    "                      sci.stats.norm.cdf(b[rating - 2] + model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along the same vein, we calculate the gradient for a single row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# by hand derived gradient\n",
    "def row_manual_grad(theta, row, b, I, J, K, R):\n",
    "    grad = np.zeros(theta.shape[0])\n",
    "    \n",
    "\n",
    "    rating = row[0]\n",
    "    i = row[1]\n",
    "    j = row[2]\n",
    "\n",
    "    # the model for the latent variable\n",
    "    u_i = theta[K * i:K*(i+1)]\n",
    "    v_j = theta[(I + j) * K:(I + j + 1) * K]\n",
    "    a_i = theta[(I + J) * K + i]\n",
    "    b_j = theta[(I + J) * K + I + j]\n",
    "    g = theta[(I + J) * K + I + J]\n",
    "    beta = theta[(I + J) * K + I + J + 1:]\n",
    "\n",
    "    # some asserts for the sizes\n",
    "    assert len(u_i) == K\n",
    "    assert len(v_j) == K\n",
    "\n",
    "    model = np.dot(u_i, v_j)\n",
    "    if rating == R:\n",
    "        q = 1 - sci.stats.norm.pdf(b[rating - 2] + model) / (1 - sci.stats.norm.cdf(b[rating - 2] + model))\n",
    "    elif rating == 1:\n",
    "        q = sci.stats.norm.pdf(b[rating - 1] + model) / sci.stats.norm.cdf(b[rating - 1] + model)\n",
    "    else:\n",
    "        q = sci.stats.norm.pdf(b[rating-2] + model) - sci.stats.norm.pdf(b[rating - 1] + model) / (sci.stats.norm.cdf(b[rating-2] + model) - sci.stats.norm.cdf(b[rating - 1] + model))\n",
    "\n",
    "    # compute derivatives\n",
    "    grad_u_i = q * v_j\n",
    "    grad_v_j = q * u_i\n",
    "    grad_a_i = q \n",
    "    grad_b_j = q\n",
    "    grad_g = q\n",
    "#         grad_beta = q * Xfeat[j, :]\n",
    "\n",
    "    grad[K * i:K*(i+1)] = grad_u_i\n",
    "    grad[(I + j) * K:(I + j + 1) * K] = grad_v_j\n",
    "    grad[(I + J) * K + i] = grad_a_i\n",
    "    grad[(I + J) * K + I + j] = grad_b_j\n",
    "    grad[(I + J) * K + I + J] = grad_g\n",
    "#         grad[(I + J) * K + I + J + 1:] += grad_beta\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two functions below use the autograd library to get the gradient of our log likelihood functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gradrll = grad(rowloglikelihood)\n",
    "gradll = grad(loglikelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get a benchmark for how fast it runs serially, using the gradient we calculated manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.05495333  -0.05574243  -0.07264744 ...,  -0.03478058  -0.0502744\n",
      " -34.01143916]\n",
      "Time taken: 58.311484 s\n",
      "-834201.329185\n"
     ]
    }
   ],
   "source": [
    "# one epoch of sgd!\n",
    "\n",
    "theta0 = theta.copy()\n",
    "\n",
    "# combined learning_rate * scale_factor\n",
    "alpha = 0.05\n",
    "\n",
    "t=time.time()\n",
    "for row in Xrat:\n",
    "    # update theta0 according to current row\n",
    "    theta0 += alpha * row_manual_grad(theta0, row, buckets, I, J, K, R)\n",
    "print theta0\n",
    "\n",
    "print \"Time taken: %f s\" % (time.time()-t)\n",
    "print loglikelihood(theta0, Xrat, buckets, I, J, K, R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallelize! Stick Xrat into an RDD, copy over the theta from before. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:703)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:702)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:702)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1514)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1438)\n\tat org.apache.spark.SparkContext$$anonfun$stop$7.apply$mcV$sp(SparkContext.scala:1724)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1185)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:1723)\n\tat org.apache.spark.SparkContext$$anonfun$3.apply$mcV$sp(SparkContext.scala:587)\n\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:264)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:234)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:234)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:234)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1699)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:234)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:234)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:234)\n\tat scala.util.Try$.apply(Try.scala:161)\n\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:234)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:216)\n\tat org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1919)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:905)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:904)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-0a84eb6a5681>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#And then compute the gradient for each row individually, and the sum it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mptheta1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxrat_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mrow_manual_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbuckets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mloglikelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mptheta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXrat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuckets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mmean\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;36m2.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m         \"\"\"\n\u001b[0;32m-> 1153\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvariance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mstats\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1014\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mleft_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmergeStats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_counter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1016\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mStatCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mredFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1017\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuckets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    797\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    800\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    771\u001b[0m         \"\"\"\n\u001b[1;32m    772\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[0;32m--> 538\u001b[0;31m                 self.target_id, self.name)\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    299\u001b[0m                     \u001b[0;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:703)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:702)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:702)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1514)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1438)\n\tat org.apache.spark.SparkContext$$anonfun$stop$7.apply$mcV$sp(SparkContext.scala:1724)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1185)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:1723)\n\tat org.apache.spark.SparkContext$$anonfun$3.apply$mcV$sp(SparkContext.scala:587)\n\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:264)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:234)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:234)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:234)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1699)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:234)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:234)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:234)\n\tat scala.util.Try$.apply(Try.scala:161)\n\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:234)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:216)\n\tat org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1919)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:905)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:904)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "xrat_rdd=sc.parallelize(Xrat)\n",
    "\n",
    "# Get a fresh theta\n",
    "theta1=theta.copy()\n",
    "file=open(\"out.txt\",\"w\")\n",
    "#And then compute the gradient for each row individually, and the sum it\n",
    "t=time.time()\n",
    "ptheta1=xrat_rdd.map(lambda x: alpha*row_manual_grad(theta1,x,buckets, I, J, K, R)).mean()\n",
    "tot=time.time()-t\n",
    "file.write(tot+\"\\n\")\n",
    "file.write(loglikelihood(ptheta1, Xrat, buckets, I, J, K, R))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That log likelihood is pretty awful though. Lets test it by breaking it up into subarrays of size S and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A function we pass to the array to calculate the updated thetas from each subarray\n",
    "\n",
    "def n_row_sgd(theta,subX, buckets, I, J, K, R):\n",
    "    for row in subX:\n",
    "        # update theta0 according to current row\n",
    "        theta += alpha * gradrll(theta, row, buckets, I, J, K, R)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 2] No such file or directory: '/private/var/folders/hm/mnf_gkh50zv3ptc6xqchtyh40000gn/T/spark-a3b0825c-198a-4a2d-8056-ba71ba6bcb4d/pyspark-06f5b308-c291-49d0-8c36-61a3f9784d3e/tmptJURPL'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-244-d516c728d022>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#And then parallelize it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msplit_xrat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_xrat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Get a fresh theta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/neilchainani/Documents/CSE/spark-1.5.0/python/pyspark/context.pyc\u001b[0m in \u001b[0;36mparallelize\u001b[0;34m(self, c, numSlices)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;31m# because it sends O(n) Py4J commands.  As an alternative, serialized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;31m# objects are written to a file and loaded through textFile().\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mtempFile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNamedTemporaryFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_temp_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0;31m# Make sure we distribute data evenly if it's smaller than self.batchSize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"__len__\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/neilchainani/anaconda/lib/python2.7/tempfile.pyc\u001b[0m in \u001b[0;36mNamedTemporaryFile\u001b[0;34m(mode, bufsize, suffix, prefix, dir, delete)\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mO_TEMPORARY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_mkstemp_inner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfdopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/neilchainani/anaconda/lib/python2.7/tempfile.pyc\u001b[0m in \u001b[0;36m_mkstemp_inner\u001b[0;34m(dir, pre, suf, flags)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m             \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m             \u001b[0m_set_cloexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 2] No such file or directory: '/private/var/folders/hm/mnf_gkh50zv3ptc6xqchtyh40000gn/T/spark-a3b0825c-198a-4a2d-8056-ba71ba6bcb4d/pyspark-06f5b308-c291-49d0-8c36-61a3f9784d3e/tmptJURPL'"
     ]
    }
   ],
   "source": [
    "S=20\n",
    "#Split the array into subarrays of size n\n",
    "split_xrat=np.split(Xrat,Xrat.shape[0]/size)\n",
    "\n",
    "#And then parallelize it\n",
    "split_xrat = sc.parallelize(split_xrat,5)\n",
    "\n",
    "# Get a fresh theta\n",
    "theta2=theta.copy()\n",
    "\n",
    "# Run the sgd!\n",
    "\n",
    "ptheta2=split_xrat.map(lambda subX:n_row_sgd(theta2, subX, buckets, I, J, K, R)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Overall Todos:\n",
    "# # add Documentation, code in MLib style\n",
    "\n",
    "# # This shall be designed for modelling ratings\n",
    "# # assume we are given n users i =1, ..., I\n",
    "# # that rated m items j= 1, ..., J\n",
    "# # Y_n ~ u_i^Tv_j + a_i + b_j + g + X_n * w\n",
    "# # u_i, v_j are K-dimensional latent variable vectors\n",
    "# # a_i, b_j, g are user/item/global wise biases (latent)\n",
    "# # X_n represents the n-th data row with L features\n",
    "# # w is the weight vector we want to train for\n",
    "# # Thus, in total our model trains the parameter vector\n",
    "# # theta = (u_1, ..., u_I, v_1, ..., v_J, a_1, ..., a_I, b_1, ..., b_J, g, w)\n",
    "# class CumulativeModel:\n",
    "    \n",
    "    \n",
    "#     def __init__(self, buckets = None, modelType = 'logistic', spark=False, numLatentFactors=0):\n",
    "#         # add here check for correct model types\n",
    "#         # logistic, probit, minev, maxev\n",
    "#         self.mtype = modelType\n",
    "#         self.useSpark = spark\n",
    "#         self.K = numLatentFactors\n",
    "#         self.buckets = buckets\n",
    "        \n",
    "#     # add here sgd parameters\n",
    "#     def fit(self, X, y, ...):\n",
    "#         # first set buckets if necessary\n",
    "#         if self.buckets is None:\n",
    "#             warning()\n",
    "        \n",
    "#     # make a prediction\n",
    "#     def predict(self, X, y, ...):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
