{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import scipy as sc\n",
    "import autograd.scipy as sci  # Thinly-wrapped scipy\n",
    "import autograd.numpy as np  # Thinly-wrapped numpy\n",
    "from autograd import grad\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import os\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x1046d18d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File ../data/ratings.csv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-75200cadb57b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# load data using pandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/ratings.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# uid = user id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# jid = joke id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/neilchainani/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, dialect, compression, doublequote, escapechar, quotechar, quoting, skipinitialspace, lineterminator, header, index_col, names, prefix, skiprows, skipfooter, skip_footer, na_values, na_fvalues, true_values, false_values, delimiter, converters, dtype, usecols, engine, delim_whitespace, as_recarray, na_filter, compact_ints, use_unsigned, low_memory, buffer_lines, warn_bad_lines, error_bad_lines, keep_default_na, thousands, comment, decimal, parse_dates, keep_date_col, dayfirst, date_parser, memory_map, float_precision, nrows, iterator, chunksize, verbose, encoding, squeeze, mangle_dupe_cols, tupleize_cols, infer_datetime_format, skip_blank_lines)\u001b[0m\n\u001b[1;32m    472\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/neilchainani/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/neilchainani/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_options_with_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/neilchainani/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    703\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/neilchainani/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:3173)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:5912)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: File ../data/ratings.csv does not exist"
     ]
    }
   ],
   "source": [
    "# # load joke data including features\n",
    "\n",
    "# # load data using pandas\n",
    "# df = pd.read_csv('../data/ratings.csv', sep=' ', header=None)\n",
    "# # uid = user id\n",
    "# # jid = joke id\n",
    "# df.columns = ['uid', 'jid', 'rating']\n",
    "\n",
    "\n",
    "# # Let N be the number of users, M be the number of jokes\n",
    "# # use +1 for easier referencing later\n",
    "# I = df.uid.max() + 1\n",
    "# J = df.jid.max() + 1\n",
    "\n",
    "# # for this problem, we do not need uid --> remove column!\n",
    "# #df = df[['rating', 'jid']]\n",
    "\n",
    "# # import joke features Jf\n",
    "# dfJ = pd.read_csv('../data/features.csv', sep=' ', header=None)\n",
    "# wrange = ['j'+str(w) for w in range(151)]\n",
    "# dfJ.columns = wrange\n",
    "# dfJ.head()\n",
    "\n",
    "# # convert to numpy matrix Jf\n",
    "# Jf = dfJ.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('../data/musicdata.csv',header=None)\n",
    "df.columns=['uid', 'aid', 'rating']\n",
    "\n",
    "# Let N be the number of users, M be the number of jokes\n",
    "# use +1 for easier referencing later\n",
    "I = df.uid.max() + 1\n",
    "J = df.aid.max() + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loglikelihood is given as\n",
    "$$ \\mathcal{L}(\\beta \\; \\vert \\; \\mathcal{D}, \\theta) = \\sum_{i=1}^n \\log \\left( F_\\epsilon(\\theta_{Y_i} + X_i^T \\beta) - F_\\epsilon(\\theta_{Y_i-1} + X_i^T\\beta) \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the log-likelihood function\n",
    "# Xrat array consisting of (rating, user, item)\n",
    "# Xfeat array indexable by the item column of Xrat which delivers item features\n",
    "# theta is the parameter vector\n",
    "# theta = (u_1, ..., u_I, v_1, ..., v_J, a_1, ..., a_I, b_1, ..., b_J, g, beta_1, ..., beta_L)\n",
    "# i.e. theta has length (I + J) * K + I + J + 1 + L\n",
    "# with u_i, v_j \\in \\mathbb{R}^K\n",
    "# b stores the buckets for R categories (i.e. we have R-1 elements in b)\n",
    "# i.e. in b we store b_1, b_2, ..., b_{R-1} for R different categories\n",
    "def loglikelihood(theta, Xrat, b, I, J, K, R):\n",
    "    \n",
    "    \n",
    "    # first implement the easy latent model using probit...\n",
    "    llsum = 0\n",
    "    for row in xrange(Xrat.shape[0]):\n",
    "        rating = Xrat[row, 0]\n",
    "        i = Xrat[row, 1]\n",
    "        j = Xrat[row, 2]\n",
    "\n",
    "        # asserts for the indices i, j & rating\n",
    "        assert i < I and i >= 0\n",
    "        assert j < J and j >= 0\n",
    "        assert rating > 0 and rating <= R\n",
    "\n",
    "        # the model for the latent variable\n",
    "        u_i = theta[K * i:K*(i+1)]\n",
    "        v_j = theta[(I + j) * K:(I + j + 1) * K]\n",
    "        a_i = theta[(I + J) * K + i]\n",
    "        b_j = theta[(I + J) * K + I + j]\n",
    "        g = theta[(I + J) * K + I + J]\n",
    "        beta = theta[(I + J) * K + I + J + 1:]\n",
    "\n",
    "        # some asserts for the sizes\n",
    "        assert len(u_i) == K\n",
    "        assert len(v_j) == K\n",
    "\n",
    "        # the full model (reduce if necessary)\n",
    "        # model with features does not work yet...\n",
    "#         model = np.dot(u_i, v_j) + a_i + b_j + g + np.dot(Xfeat[j, :], beta) \n",
    "\n",
    "        # here are some other choices\n",
    "\n",
    "        # easy model using features only\n",
    "        # model = np.dot(Xfeat[j, :], beta)\n",
    "\n",
    "        # model using features + biases\n",
    "        # model = np.dot(Xfeat[j, :], beta) + a_i + b_j + g\n",
    "\n",
    "        # model using latent factors for user/item\n",
    "        model = np.dot(u_i, v_j)\n",
    "\n",
    "        # model using latent factors for user/item and biases\n",
    "        # model = np.dot(u_i, v_j) + a_i + b_j + g\n",
    "\n",
    "        # the ordinal regression part\n",
    "        # if rating is 1 or R we have a special case\n",
    "        # another possibility would be to use instead of the ifelse construction\n",
    "        # dummy values like +/- 9999 for infty\n",
    "        # note the additional -1 due to space saving!\n",
    "        if rating == R:\n",
    "            # note F(infty) = 0 (mathematically not rigourous, limit is more correct)\n",
    "            llsum += np.log(sci.stats.norm.cdf(b[rating - 2] + model))\n",
    "        elif rating == 1:\n",
    "            # note F(-infty) = 0\n",
    "            llsum += np.log(1 - sci.stats.norm.cdf(b[rating - 1] + model))\n",
    "        else:\n",
    "            llsum += np.log(sci.stats.norm.cdf(b[rating - 1] + model) - sci.stats.norm.cdf(b[rating - 2] + model))\n",
    "    return llsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-inf"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing the gradient\n",
    "\n",
    "# create sample df\n",
    "dftouse = df[['rating', 'uid', 'aid']].head(2000)\n",
    "# as in the given dataset the indices are 1,...,I and 1, ..., J\n",
    "# adjust jokes & uid s.t. they serve the 0, ..., I-1 and 0, ..., J-1 space \n",
    "dftouse['uid'] = dftouse['uid'] - 1\n",
    "dftouse['aid'] = dftouse['aid'] - 1\n",
    "\n",
    "# transform ratings to range 1, ..., R\n",
    "rating_vals = np.arange(1,dftouse.rating.max()+1)\n",
    "minR = dftouse.rating.min()\n",
    "dftouse['rating'] = dftouse['rating'] - minR + 1\n",
    "R = len(rating_vals)\n",
    "\n",
    "\n",
    "# create buckets as midpoints\n",
    "buckets = 0.5 * (rating_vals[1:] + rating_vals[:-1])\n",
    "\n",
    "# get length I, J\n",
    "I = dftouse.uid.max() + 1\n",
    "J = dftouse.aid.max() + 1\n",
    "\n",
    "# define some K\n",
    "K = 2\n",
    "\n",
    "# convert to numpy data matrix\n",
    "Xrat = np.array(dftouse)\n",
    "# Xfeat = Jf\n",
    "\n",
    "# create dummy theta vector (all zeros)\n",
    "# L = Xfeat.shape[1]\n",
    "theta = np.zeros((I + J) * K + I + J + 1)\n",
    "\n",
    "# init theta vector with some random values\n",
    "theta = np.random.normal(size=theta.shape[0], loc=0., scale=0.1)\n",
    "\n",
    "# compute log likelihood\n",
    "loglikelihood(theta, Xrat, buckets, I, J, K, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## the rowlikelihood for sgd\n",
    "def rowloglikelihood(theta, row, Xfeat, b, I, J, K, R):\n",
    "    L = Xfeat.shape[1]\n",
    "    rating = row[0]\n",
    "    i = row[1]\n",
    "    j = row[2]\n",
    "\n",
    "    # asserts for the indices i, j & rating\n",
    "    assert i < I and i >= 0\n",
    "    assert j < J and j >= 0\n",
    "    assert Xfeat.shape[0] == J\n",
    "    assert rating > 0 and rating <= R\n",
    "\n",
    "    # the model for the latent variable\n",
    "    u_i = theta[K * i:K*(i+1)]\n",
    "    v_j = theta[(I + j) * K:(I + j + 1) * K]\n",
    "    a_i = theta[(I + J) * K + i]\n",
    "    b_j = theta[(I + J) * K + I + j]\n",
    "    g = theta[(I + J) * K + I + J]\n",
    "    beta = theta[(I + J) * K + I + J + 1:]\n",
    "\n",
    "    # some asserts for the sizes\n",
    "    assert len(u_i) == K\n",
    "    assert len(v_j) == K\n",
    "    assert len(beta) == L\n",
    "\n",
    "    # the full model (reduce if necessary)\n",
    "    # model with features does not work yet...\n",
    "    model = np.dot(u_i, v_j) + a_i + b_j + g + np.dot(Xfeat[j, :], beta) \n",
    "    \n",
    "    # here are some other choices\n",
    "    \n",
    "    # easy model using features only\n",
    "    # model = np.dot(Xfeat[j, :], beta)\n",
    "    \n",
    "    # model using features + biases\n",
    "    # model = np.dot(Xfeat[j, :], beta) + a_i + b_j + g\n",
    "    \n",
    "    # model using latent factors for user/item\n",
    "    # model = np.dot(u_i, v_j)\n",
    "    \n",
    "    # model using latent factors for user/item and biases\n",
    "    # model = np.dot(u_i, v_j) + a_i + b_j + g\n",
    "\n",
    "    # the ordinal regression part\n",
    "    # if rating is 1 or R we have a special case\n",
    "    # another possibility would be to use instead of the ifelse construction\n",
    "    # dummy values like +/- 9999 for infty\n",
    "    # note the additional -1 due to space saving!\n",
    "    if rating == R:\n",
    "        # note F(infty) = 0 (mathematically not rigourous, limit is more correct)\n",
    "        return np.log(sci.stats.norm.cdf(b[rating - 2] + model))\n",
    "    elif rating == 1:\n",
    "        # note F(-infty) = 0\n",
    "        return np.log(1 - sci.stats.norm.cdf(b[rating - 1] + model))\n",
    "    else:\n",
    "        return np.log(sci.stats.norm.cdf(b[rating - 1] + model) - sci.stats.norm.cdf(b[rating - 2] + model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        , ..., -3.65815073,\n",
       "        0.        , -3.65815073])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use autograd to get a gradient out of the rowlikelihood\n",
    "gradrll = grad(rowloglikelihood)\n",
    "gradll = grad(loglikelihood)\n",
    "\n",
    "# check that gradient has the right length and output sample\n",
    "assert len(gradrll(theta, Xrat[5], Xfeat, buckets, I, J, K, R)) == (I + J) * K + I + J + 1 + L\n",
    "gradrll(theta, Xrat[5], Xfeat, buckets, I, J, K, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-8065.0295793309624, -8065.0295793309624)\n",
      "[ -9.38462676e-03  -1.12376626e-02   0.00000000e+00 ...,  -2.50382169e+02\n",
      "  -3.44893040e+02  -4.16101994e+03]\n",
      "[ -9.38462676e-03  -1.12376626e-02   0.00000000e+00 ...,  -2.50382169e+02\n",
      "  -3.44893040e+02  -4.16101994e+03]\n"
     ]
    }
   ],
   "source": [
    "# test the row likelihood by comparing to the full one\n",
    "llsum = 0.\n",
    "for row in Xrat:\n",
    "    llsum += rowloglikelihood(theta, row, Xfeat, buckets, I, J, K, R)\n",
    "print (llsum, loglikelihood(theta, Xrat, Xfeat, buckets, I, J, K, R))\n",
    "\n",
    "# do analogously the gradient test...\n",
    "gsum = 0.\n",
    "for row in Xrat:\n",
    "    gsum += gradrll(theta, row, Xfeat, buckets, I, J, K, R)\n",
    "print gsum\n",
    "print gradll(theta, Xrat, Xfeat, buckets, I, J, K, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0.             0.             0.         ...,  -350.56791086\n",
      "  -425.33674269 -4612.06896786]\n"
     ]
    }
   ],
   "source": [
    "# do sgd approximation\n",
    "M = 100 # choose M samples\n",
    "gsum = 0.\n",
    "for row in np.random.randint(Xrat.shape[0], size=M):\n",
    "    # don't forget scaling!\n",
    "    gsum += 1. * Xrat.shape[0] / M * gradrll(theta, Xrat[row], Xfeat, buckets, I, J, K, R)\n",
    "print gsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0050993   0.06732006  0.08307213 ..., -0.01384602 -0.11538214\n",
      " -1.40406265]\n",
      "Time taken: 7.128969 s\n",
      "-1850.51150326\n"
     ]
    }
   ],
   "source": [
    "# one epoch of sgd!\n",
    "\n",
    "theta0 = theta.copy()\n",
    "\n",
    "# combined learning_rate * scale_factor\n",
    "alpha = 0.05\n",
    "# shuffle data here!\n",
    "\n",
    "t=time.time()\n",
    "for row in Xrat:\n",
    "    # update theta0 according to current row\n",
    "    theta0 += alpha * gradrll(theta0, row, Xfeat, buckets, I, J, K, R)\n",
    "print theta0\n",
    "\n",
    "print \"Time taken: %f s\" % (time.time()-t)\n",
    "print loglikelihood(theta0, Xrat, Xfeat, buckets, I, J, K, R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 66 ms, sys: 97 ms, total: 163 ms\n",
      "Wall time: 9.55 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-4875.4270370881022"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now to parallelize! Stick Xrat into an RDD\n",
    "xrat_rdd=sc.parallelize(Xrat)\n",
    "\n",
    "# Get a fresh theta\n",
    "theta1=theta.copy()\n",
    "\n",
    "#And then compute the gradient for each row individually, and the sum it\n",
    "%time ptheta1=xrat_rdd.map(lambda x: alpha*gradrll(theta1,x,Xfeat, buckets, I, J, K, R)).mean()\n",
    "n=xrat_rdd.count()\n",
    "# subtract out (n-1)*theta1 to get final theta\n",
    "loglikelihood(ptheta1, Xrat, Xfeat, buckets, I, J, K, R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That log likelihood is pretty awful though. Lets break it up into subarrays of size 5 and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A function we pass to the array to calculate the updated thetas from each subarray\n",
    "\n",
    "def n_row_sgd(theta,subX, Xfeat, buckets, I, J, K, R):\n",
    "    for row in subX:\n",
    "        # update theta0 according to current row\n",
    "        theta += alpha * gradrll(theta, row, Xfeat, buckets, I, J, K, R)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 75.3 ms, sys: 108 ms, total: 183 ms\n",
      "Wall time: 6.68 s\n"
     ]
    }
   ],
   "source": [
    "size=5\n",
    "#Split the array into subarrays of size n\n",
    "split_xrat=np.split(Xrat,Xrat.shape[0]/size)\n",
    "\n",
    "#And then parallelize it\n",
    "split_xrat = sc.parallelize(split_xrat,5)\n",
    "\n",
    "# Get a fresh theta\n",
    "theta2=theta.copy()\n",
    "\n",
    "# Run the sgd!\n",
    "%time ptheta2=split_xrat.map(lambda subX:n_row_sgd(theta2, subX, Xfeat, buckets, I, J, K, R)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1640.866923790534"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loglikelihood(ptheta2, Xrat, Xfeat, buckets, I, J, K, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Overall Todos:\n",
    "# # add Documentation, code in MLib style\n",
    "\n",
    "# # This shall be designed for modelling ratings\n",
    "# # assume we are given n users i =1, ..., I\n",
    "# # that rated m items j= 1, ..., J\n",
    "# # Y_n ~ u_i^Tv_j + a_i + b_j + g + X_n * w\n",
    "# # u_i, v_j are K-dimensional latent variable vectors\n",
    "# # a_i, b_j, g are user/item/global wise biases (latent)\n",
    "# # X_n represents the n-th data row with L features\n",
    "# # w is the weight vector we want to train for\n",
    "# # Thus, in total our model trains the parameter vector\n",
    "# # theta = (u_1, ..., u_I, v_1, ..., v_J, a_1, ..., a_I, b_1, ..., b_J, g, w)\n",
    "# class CumulativeModel:\n",
    "    \n",
    "    \n",
    "#     def __init__(self, buckets = None, modelType = 'logistic', spark=False, numLatentFactors=0):\n",
    "#         # add here check for correct model types\n",
    "#         # logistic, probit, minev, maxev\n",
    "#         self.mtype = modelType\n",
    "#         self.useSpark = spark\n",
    "#         self.K = numLatentFactors\n",
    "#         self.buckets = buckets\n",
    "        \n",
    "#     # add here sgd parameters\n",
    "#     def fit(self, X, y, ...):\n",
    "#         # first set buckets if necessary\n",
    "#         if self.buckets is None:\n",
    "#             warning()\n",
    "        \n",
    "#     # make a prediction\n",
    "#     def predict(self, X, y, ...):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
