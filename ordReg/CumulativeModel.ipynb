{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import scipy as sc\n",
    "import autograd.scipy as sc  # Thinly-wrapped scipy\n",
    "import autograd.numpy as np  # Thinly-wrapped numpy\n",
    "from autograd import grad\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import os\n",
    "import pandas as pd\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load joke data including features\n",
    "\n",
    "# load data using pandas\n",
    "df = pd.read_csv('../data/ratings.csv', sep=' ', header=None)\n",
    "# uid = user id\n",
    "# jid = joke id\n",
    "df.columns = ['uid', 'jid', 'rating']\n",
    "\n",
    "\n",
    "# Let N be the number of users, M be the number of jokes\n",
    "# use +1 for easier referencing later\n",
    "I = df.uid.max() + 1\n",
    "J = df.jid.max() + 1\n",
    "\n",
    "# for this problem, we do not need uid --> remove column!\n",
    "#df = df[['rating', 'jid']]\n",
    "\n",
    "# import joke features Jf\n",
    "dfJ = pd.read_csv('../data/features.csv', sep=' ', header=None)\n",
    "wrange = ['j'+str(w) for w in range(0, 151)]\n",
    "dfJ.columns = wrange\n",
    "dfJ.head()\n",
    "\n",
    "# convert to numpy matrix Jf\n",
    "Jf = dfJ.values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loglikelihood is given as\n",
    "$$ \\mathcal{L}(\\beta \\; \\vert \\; \\mathcal{D}, \\theta) = \\sum_{i=1}^n \\log \\left( F_\\epsilon(\\theta_{Y_i} + X_i^T \\beta) - F_\\epsilon(\\theta_{Y_i-1} + X_i^T\\beta) \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the log-likelihood function\n",
    "# Xrat array consisting of (rating, user, item)\n",
    "# Xfeat array indexable by the item column of Xrat which delivers item features\n",
    "# theta is the parameter vector\n",
    "# theta = (u_1, ..., u_I, v_1, ..., v_J, a_1, ..., a_I, b_1, ..., b_J, g, beta_1, ..., beta_L)\n",
    "# i.e. theta has length (I + J) * K + I + J + 1 + L\n",
    "# with u_i, v_j \\in \\mathbb{R}^K\n",
    "# b stores the buckets for R categories (i.e. we have R-1 elements in b)\n",
    "# i.e. in b we store b_1, b_2, ..., b_{R-1} for R different categories\n",
    "def loglikelihood(theta, Xrat, Xfeat, b, I, J, K, R):\n",
    "    \n",
    "    L = Xfeat.shape[1]\n",
    "    \n",
    "    # first implement the easy latent model using probit...\n",
    "    llsum = 0\n",
    "    for row in xrange(Xrat.shape[0]):\n",
    "        rating = Xrat[row, 0]\n",
    "        i = Xrat[row, 1]\n",
    "        j = Xrat[row, 2]\n",
    "\n",
    "        # asserts for the indices i, j & rating\n",
    "        assert i < I and i >= 0\n",
    "        assert j < J and j >= 0\n",
    "        assert Xfeat.shape[0] == J\n",
    "        assert rating > 0 and rating <= R\n",
    "\n",
    "        # the model for the latent variable\n",
    "        u_i = theta[K * i:K*(i+1)]\n",
    "        v_j = theta[(I + j) * K:(I + j + 1) * K]\n",
    "        a_i = theta[(I + J) * K + i]\n",
    "        b_j = theta[(I + J) * K + I + j]\n",
    "        g = theta[(I + J) * K + I + J]\n",
    "        beta = theta[(I + J) * K + I + J + 1:]\n",
    "\n",
    "        # some asserts for the sizes\n",
    "        assert len(u_i) == K\n",
    "        assert len(v_j) == K\n",
    "        assert len(beta) == L\n",
    "\n",
    "        # the full model (reduce if necessary)\n",
    "        # model with features does not work yet...\n",
    "        model = np.dot(u_i, v_j) + a_i + b_j + g + np.dot(Xfeat[j, :], beta) \n",
    "\n",
    "        # here are some other choices\n",
    "\n",
    "        # easy model using features only\n",
    "        # model = np.dot(Xfeat[j, :], beta)\n",
    "\n",
    "        # model using features + biases\n",
    "        # model = np.dot(Xfeat[j, :], beta) + a_i + b_j + g\n",
    "\n",
    "        # model using latent factors for user/item\n",
    "        # model = np.dot(u_i, v_j)\n",
    "\n",
    "        # model using latent factors for user/item and biases\n",
    "        # model = np.dot(u_i, v_j) + a_i + b_j + g\n",
    "\n",
    "        # the ordinal regression part\n",
    "        # if rating is 1 or R we have a special case\n",
    "        # another possibility would be to use instead of the ifelse construction\n",
    "        # dummy values like +/- 9999 for infty\n",
    "        # note the additional -1 due to space saving!\n",
    "#         if rating == R:\n",
    "#             # note F(infty) = 0 (mathematically not rigourous, limit is more correct)\n",
    "#             llsum += np.log(sc.stats.norm.cdf(b[rating - 2] + model))\n",
    "#         elif rating == 1:\n",
    "#             # note F(-infty) = 0\n",
    "#             llsum += np.log(1 - sc.stats.norm.cdf(b[rating - 1] + model))\n",
    "#         else:\n",
    "#             llsum += np.log(sc.stats.norm.cdf(b[rating - 1] + model) - sc.stats.norm.cdf(b[rating - 2] + model))\n",
    "            \n",
    "        llsum += np.log(sc.stats.norm.cdf(b[rating] + model) - sc.stats.norm.cdf(b[rating - 1] + model))\n",
    "    return llsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1380.2074667158931"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing the gradient\n",
    "\n",
    "# create sample df\n",
    "dftouse = df[['rating', 'uid', 'jid']].head(200)\n",
    "\n",
    "# as in the given dataset the indices are 1,...,I and 1, ..., J\n",
    "# adjust jokes & uid s.t. they serve the 0, ..., I-1 and 0, ..., J-1 space \n",
    "dftouse['uid'] = dftouse['uid'] - 1\n",
    "dftouse['jid'] = dftouse['jid'] - 1\n",
    "\n",
    "# transform ratings to range 1, ..., R\n",
    "rating_vals = np.sort(pd.unique(dftouse['rating'].values.ravel()))\n",
    "minR = dftouse.rating.min()\n",
    "dftouse['rating'] = dftouse['rating'] - minR + 1\n",
    "R = len(rating_vals)\n",
    "\n",
    "# create buckets as midpoints\n",
    "buckets = 0.5 * (rating_vals[1:] + rating_vals[:-1])\n",
    "# buckets per hand ( 4 needed)\n",
    "# 999 is infty\n",
    "infty = 999\n",
    "buckets = np.array([-infty, 1.8, 2.2, 3.3, 4.5, infty])\n",
    "\n",
    "# get length I, J\n",
    "I = dftouse.uid.max() + 1\n",
    "J = dftouse.jid.max() + 1\n",
    "\n",
    "# define some K\n",
    "K = 2\n",
    "\n",
    "# convert to numpy data matrix\n",
    "Xrat = np.array(dftouse)\n",
    "Xfeat = Jf\n",
    "\n",
    "# create dummy theta vector (all zeros)\n",
    "L = Xfeat.shape[1]\n",
    "theta = np.zeros((I + J) * K + I + J + 1 + L)\n",
    "\n",
    "# init theta vector with some random values\n",
    "theta = np.random.normal(size=theta.shape[0], loc=0., scale=0.1)\n",
    "\n",
    "# compute log likelihood\n",
    "loglikelihood(theta, Xrat, Xfeat, buckets, I, J, K, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# by hand derived gradient\n",
    "def manual_grad(theta, Xrat, Xfeat, b, I, J, K, R):\n",
    "    grad = np.zeros(theta.shape[0])\n",
    "    \n",
    "    # go over samples and make update according to them!\n",
    "    for n in xrange(Xrat.shape[0]):\n",
    "        \n",
    "        rating = Xrat[n, 0]\n",
    "        i = Xrat[n, 1]\n",
    "        j = Xrat[n, 2]\n",
    "        \n",
    "        # the model for the latent variable\n",
    "        u_i = theta[K * i:K*(i+1)]\n",
    "        v_j = theta[(I + j) * K:(I + j + 1) * K]\n",
    "        a_i = theta[(I + J) * K + i]\n",
    "        b_j = theta[(I + J) * K + I + j]\n",
    "        g = theta[(I + J) * K + I + J]\n",
    "        beta = theta[(I + J) * K + I + J + 1:]\n",
    "\n",
    "        # some asserts for the sizes\n",
    "        assert len(u_i) == K\n",
    "        assert len(v_j) == K\n",
    "        assert len(beta) == L\n",
    "        \n",
    "        # compute q_n term\n",
    "        # the full model (reduce if necessary)\n",
    "        # model with features does not work yet...\n",
    "        model = np.dot(u_i, v_j) + a_i + b_j + g + np.dot(Xfeat[j, :], beta) \n",
    "        q = 0\n",
    "        \n",
    "        # note that this works only for particular pdfs!!!! i.e. they are approaching 0!\n",
    "        q = sc.stats.norm.pdf(b[rating] + model) - sc.stats.norm.pdf(b[rating - 1] + model) / \\\n",
    "            (sc.stats.norm.cdf(b[rating] + model) - sc.stats.norm.cdf(b[rating - 1] + model))\n",
    "        \n",
    "        # compute derivatives\n",
    "        grad_u_i = q * v_j\n",
    "        grad_v_j = q * u_i\n",
    "        grad_a_i = q \n",
    "        grad_b_j = q\n",
    "        grad_g = q\n",
    "        grad_beta = q * Xfeat[j, :]\n",
    "        \n",
    "        grad[K * i:K*(i+1)] += grad_u_i\n",
    "        grad[(I + j) * K:(I + j + 1) * K] += grad_v_j\n",
    "        grad[(I + J) * K + i] += grad_a_i\n",
    "        grad[(I + J) * K + I + j] += grad_b_j\n",
    "        grad[(I + J) * K + I + J] += grad_g\n",
    "        grad[(I + J) * K + I + J + 1:] += grad_beta\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0.            0.            0.         ...,  -65.13756974  -70.621929\n",
      " -612.09649219]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([   0.        ,    0.        ,    0.        , ...,  -57.59335142,\n",
       "        -74.76345567, -670.64776854])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradll = grad(loglikelihood)\n",
    "\n",
    "# check that gradient has the right length and output sample\n",
    "#assert len(gradll(theta, Xrat, Xfeat, buckets, I, J, K, R)) == (I + J) * K + I + J + 1 + L\n",
    "print gradll(theta, Xrat, Xfeat, buckets, I, J, K, R)\n",
    "# init theta vector with some random values\n",
    "theta = np.random.normal(size=theta.shape[0], loc=0., scale=0.1)\n",
    "manual_grad(theta, Xrat, Xfeat, buckets, I, J, K, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the rowlikelihood for sgd\n",
    "def rowloglikelihood(theta, row, Xrat, Xfeat, b, I, J, K, R):\n",
    "    L = Xfeat.shape[1]\n",
    "    rating = Xrat[row, 0]\n",
    "    i = Xrat[row, 1]\n",
    "    j = Xrat[row, 2]\n",
    "\n",
    "    # asserts for the indices i, j & rating\n",
    "    assert i < I and i >= 0\n",
    "    assert j < J and j >= 0\n",
    "    assert Xfeat.shape[0] == J\n",
    "    assert rating > 0 and rating <= R\n",
    "\n",
    "    # the model for the latent variable\n",
    "    u_i = theta[K * i:K*(i+1)]\n",
    "    v_j = theta[(I + j) * K:(I + j + 1) * K]\n",
    "    a_i = theta[(I + J) * K + i]\n",
    "    b_j = theta[(I + J) * K + I + j]\n",
    "    g = theta[(I + J) * K + I + J]\n",
    "    beta = theta[(I + J) * K + I + J + 1:]\n",
    "\n",
    "    # some asserts for the sizes\n",
    "    assert len(u_i) == K\n",
    "    assert len(v_j) == K\n",
    "    assert len(beta) == L\n",
    "\n",
    "    # the full model (reduce if necessary)\n",
    "    # model with features does not work yet...\n",
    "    model = np.dot(u_i, v_j) + a_i + b_j + g + np.dot(Xfeat[j, :], beta) \n",
    "    \n",
    "    # here are some other choices\n",
    "    \n",
    "    # easy model using features only\n",
    "    # model = np.dot(Xfeat[j, :], beta)\n",
    "    \n",
    "    # model using features + biases\n",
    "    # model = np.dot(Xfeat[j, :], beta) + a_i + b_j + g\n",
    "    \n",
    "    # model using latent factors for user/item\n",
    "    # model = np.dot(u_i, v_j)\n",
    "    \n",
    "    # model using latent factors for user/item and biases\n",
    "    # model = np.dot(u_i, v_j) + a_i + b_j + g\n",
    "\n",
    "    # the ordinal regression part\n",
    "    # if rating is 1 or R we have a special case\n",
    "    # another possibility would be to use instead of the ifelse construction\n",
    "    # dummy values like +/- 9999 for infty\n",
    "    # note the additional -1 due to space saving!\n",
    "    if rating == R:\n",
    "        # note F(infty) = 0 (mathematically not rigourous, limit is more correct)\n",
    "        return np.log(sc.stats.norm.cdf(b[rating - 2] + model))\n",
    "    elif rating == 1:\n",
    "        # note F(-infty) = 0\n",
    "        return np.log(1 - sc.stats.norm.cdf(b[rating - 1] + model))\n",
    "    else:\n",
    "        return np.log(sc.stats.norm.cdf(b[rating - 1] + model) - sc.stats.norm.cdf(b[rating - 2] + model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        , ..., -3.09819424,\n",
       "        0.        , -3.09819424])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use autograd to get a gradient out of the rowlikelihood\n",
    "gradrll = grad(rowloglikelihood)\n",
    "gradll = grad(loglikelihood)\n",
    "\n",
    "# check that gradient has the right length and output sample\n",
    "assert len(gradrll(theta, 5, Xrat, Xfeat, buckets, I, J, K, R)) == (I + J) * K + I + J + 1 + L\n",
    "gradrll(theta, 5, Xrat, Xfeat, buckets, I, J, K, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-7906.5050053024661, -7906.5050053024661)\n",
      "[  9.78137949e-02  -5.86879598e-02   0.00000000e+00 ...,  -2.60651665e+02\n",
      "  -3.36704814e+02  -4.09320543e+03]\n",
      "[  9.78137949e-02  -5.86879598e-02   0.00000000e+00 ...,  -2.60651665e+02\n",
      "  -3.36704814e+02  -4.09320543e+03]\n",
      "[  1.95627590e+00  -1.17375920e+00   0.00000000e+00 ...,  -1.34814650e+02\n",
      "  -3.28475006e+02  -3.84881225e+03]\n"
     ]
    }
   ],
   "source": [
    "# test the row likelihood by comparing to the full one\n",
    "llsum = 0.\n",
    "for row in xrange(Xrat.shape[0]):\n",
    "    llsum += rowloglikelihood(theta, row, Xrat, Xfeat, buckets, I, J, K, R)\n",
    "print (llsum, loglikelihood(theta, Xrat, Xfeat, buckets, I, J, K, R))\n",
    "\n",
    "# do analogously the gradient test...\n",
    "gsum = 0.\n",
    "for row in xrange(Xrat.shape[0]):\n",
    "    gsum += gradrll(theta, row, Xrat, Xfeat, buckets, I, J, K, R)\n",
    "print gsum\n",
    "print gradll(theta, Xrat, Xfeat, buckets, I, J, K, R)\n",
    "\n",
    "# do sgd approximation\n",
    "M = 100 # choose M samples\n",
    "gsum = 0.\n",
    "for row in np.random.randint(Xrat.shape[0], size=M):\n",
    "    # don't forget scaling!\n",
    "    gsum += 1. * Xrat.shape[0] / M * gradrll(theta, row, Xrat, Xfeat, buckets, I, J, K, R)\n",
    "print gsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00648461 -0.18237909  0.0618506  ...,  0.05158269  0.05723133\n",
      "  0.04121165]\n",
      "-7906.5050053\n",
      "[-0.00621295 -0.18254208  0.0618506  ...,  0.08054081  0.00643034\n",
      " -1.24587309]\n",
      "-1956.72718667\n"
     ]
    }
   ],
   "source": [
    "# one epoch of sgd!\n",
    "\n",
    "theta0 = theta.copy()\n",
    "\n",
    "# combined learning_rate * scale_factor\n",
    "alpha = 0.05\n",
    "\n",
    "print theta0\n",
    "print loglikelihood(theta0, Xrat, Xfeat, buckets, I, J, K, R)\n",
    "\n",
    "# shuffle data here!\n",
    "for row in xrange(Xrat.shape[0]):\n",
    "    # update theta0 according to current row\n",
    "    theta0 += alpha * gradrll(theta0, row, Xrat, Xfeat, buckets, I, J, K, R)\n",
    "    \n",
    "print theta0\n",
    "print loglikelihood(theta0, Xrat, Xfeat, buckets, I, J, K, R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# given a testsample we derive the class via the latent variable\n",
    "def predict(theta, Xrat, Xfeat, buckets, I, J, K, R):\n",
    "    \n",
    "    y = np.zeros(Xrat.shape[0])\n",
    "    for row in xrange(Xrat.shape[0]):\n",
    "        L = Xfeat.shape[1]\n",
    "        rating = Xrat[row, 0]\n",
    "        i = Xrat[row, 1]\n",
    "        j = Xrat[row, 2]\n",
    "    \n",
    "         # the model for the latent variable\n",
    "        u_i = theta[K * i:K*(i+1)]\n",
    "        v_j = theta[(I + j) * K:(I + j + 1) * K]\n",
    "        a_i = theta[(I + J) * K + i]\n",
    "        b_j = theta[(I + J) * K + I + j]\n",
    "        g = theta[(I + J) * K + I + J]\n",
    "        beta = theta[(I + J) * K + I + J + 1:]\n",
    "\n",
    "        # some asserts for the sizes\n",
    "        assert len(u_i) == K\n",
    "        assert len(v_j) == K\n",
    "        assert len(beta) == L\n",
    "\n",
    "        # the full model (reduce if necessary)\n",
    "        # model with features does not work yet...\n",
    "        model = np.dot(u_i, v_j) + a_i + b_j + g + np.dot(Xfeat[j, :], beta) \n",
    "        Y = -model\n",
    "        \n",
    "        # predict y based on where it lies within the buckets\n",
    "        y[row] = np.sum(Y > buckets) + 1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(predict(theta0, Xrat, Xfeat, buckets, I, J, K, R))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Overall Todos:\n",
    "# # add Documentation, code in MLib style\n",
    "\n",
    "# # This shall be designed for modelling ratings\n",
    "# # assume we are given n users i =1, ..., I\n",
    "# # that rated m items j= 1, ..., J\n",
    "# # Y_n ~ u_i^Tv_j + a_i + b_j + g + X_n * w\n",
    "# # u_i, v_j are K-dimensional latent variable vectors\n",
    "# # a_i, b_j, g are user/item/global wise biases (latent)\n",
    "# # X_n represents the n-th data row with L features\n",
    "# # w is the weight vector we want to train for\n",
    "# # Thus, in total our model trains the parameter vector\n",
    "# # theta = (u_1, ..., u_I, v_1, ..., v_J, a_1, ..., a_I, b_1, ..., b_J, g, w)\n",
    "# class CumulativeModel:\n",
    "    \n",
    "    \n",
    "#     def __init__(self, buckets = None, modelType = 'logistic', spark=False, numLatentFactors=0):\n",
    "#         # add here check for correct model types\n",
    "#         # logistic, probit, minev, maxev\n",
    "#         self.mtype = modelType\n",
    "#         self.useSpark = spark\n",
    "#         self.K = numLatentFactors\n",
    "#         self.buckets = buckets\n",
    "        \n",
    "#     # add here sgd parameters\n",
    "#     def fit(self, X, y, ...):\n",
    "#         # first set buckets if necessary\n",
    "#         if self.buckets is None:\n",
    "#             warning()\n",
    "        \n",
    "#     # make a prediction\n",
    "#     def predict(self, X, y, ...):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
