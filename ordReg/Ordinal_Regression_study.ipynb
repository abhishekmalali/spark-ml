{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An ordinal regression study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Music ratings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our computations we use a restricted version of the Yahoo Music User ratings (R1) <https://webscope.sandbox.yahoo.com/catalog.php?datatype=r>. Due to licensing terms, we can unfortunately not rehost it here. As the original dataset contains over 10 million ratings, we restrict it to ratings for only certain artists. To simplify it, we choose the 100 all time favourites as quoted by the Billboard charts <http://www.billboard.com/charts/greatest-billboard-200-artists>\n",
    "\n",
    "For 'Carrie Underwood', 'Tim Mcgraw', 'Lady Gaga', 'Justin Bieber', 'Adele', 'Miley Cyrus', 'P!nk', 'Jay Z', 'Taylor Swift' no entries exist in the Yahoo Music User Ratings as these artists became known after 2004.\n",
    "\n",
    "Thus, these artists have been excluded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import scipy as sc\n",
    "import autograd.scipy as sc  # Thinly-wrapped scipy\n",
    "import autograd.numpy as np  # Thinly-wrapped numpy\n",
    "from autograd import grad\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import os\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from parse import *\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing the Yahoo data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import billboard top 100\n",
    "dfbb = pd.read_csv('../data/billboard100.csv', header=None)\n",
    "artists = dfbb.values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load yahoo data\n",
    "artist_names_file = '../cache/ydata-ymusic-artist-names-v1_0.txt'\n",
    "\n",
    "#artists_found = []\n",
    "d_list = []\n",
    "with open(artist_names_file, 'r') as f:\n",
    "    for line in f:\n",
    "        \n",
    "        parsed_line = [el.strip() for el in line.split('\\t')]\n",
    "        \n",
    "        artist = parsed_line[1]\n",
    "        artist_id = int(parsed_line[0])\n",
    "\n",
    "        if artist in artists:\n",
    "            d_list.append(dict([('artist', artist), ('aid', artist_id)]))\n",
    "#         if artist in artists:\n",
    "#             print 'found ' + artist\n",
    "#             artists_found.append(artist)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfartists = pd.DataFrame(d_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now join on the big dataset with artist table\n",
    "#df = pd.read_csv('../cache/ydata-ymusic-user-artist-ratings-v1_0.txt', sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# setup spark\n",
    "try:\n",
    "    conf = SparkConf().setAppName('Yahoo Music Ratings')\n",
    "    spark_context = SparkContext(conf=conf)\n",
    "except:\n",
    "    print 'context already running'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = spark_context.textFile('../cache/ydata-ymusic-user-artist-ratings-v1_0.txt')\n",
    "\n",
    "# uncomment for testpurposes\n",
    "#rdd = spark_context.parallelize(rdd.take(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1 µs, total: 5 µs\n",
      "Wall time: 10 µs\n",
      "merging part-00000...\n",
      "merging part-00001...\n",
      "merging part-00002...\n",
      "merging part-00003...\n",
      "merging part-00004...\n",
      "merging part-00005...\n",
      "merging part-00006...\n",
      "merging part-00007...\n",
      "merging part-00008...\n",
      "merging part-00009...\n",
      "merging part-00010...\n",
      "merging part-00011...\n",
      "merging part-00012...\n",
      "merging part-00013...\n",
      "merging part-00014...\n",
      "merging part-00015...\n",
      "merging part-00016...\n",
      "merging part-00017...\n",
      "merging part-00018...\n",
      "merging part-00019...\n",
      "merging part-00020...\n",
      "merging part-00021...\n",
      "merging part-00022...\n",
      "merging part-00023...\n",
      "merging part-00024...\n",
      "merging part-00025...\n",
      "merging part-00026...\n",
      "merging part-00027...\n",
      "merging part-00028...\n",
      "merging part-00029...\n",
      "merging part-00030...\n",
      "merging part-00031...\n",
      "merging part-00032...\n",
      "merging part-00033...\n",
      "merging part-00034...\n",
      "merging part-00035...\n",
      "merging part-00036...\n",
      "merging part-00037...\n",
      "merging part-00038...\n",
      "merging part-00039...\n",
      "merging part-00040...\n",
      "merging part-00041...\n",
      "merging part-00042...\n",
      "merging part-00043...\n",
      "merging part-00044...\n",
      "merging part-00045...\n",
      "merging part-00046...\n",
      "merging part-00047...\n",
      "merging part-00048...\n",
      "merging part-00049...\n",
      "merging part-00050...\n",
      "merging part-00051...\n",
      "merging part-00052...\n",
      "merging part-00053...\n",
      "merging part-00054...\n",
      "merging part-00055...\n",
      "merging part-00056...\n",
      "merging part-00057...\n",
      "merging part-00058...\n",
      "merging part-00059...\n",
      "merging part-00060...\n",
      "merging part-00061...\n",
      "merging part-00062...\n",
      "merging part-00063...\n",
      "merging part-00064...\n",
      "merging part-00065...\n",
      "merging part-00066...\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "aids = dfartists.aid.values\n",
    "\n",
    "# filter data according to artists (this might take a while, ~ 12min on my Macbook)\n",
    "# first entry is user, artist, rating\n",
    "frdd = rdd.map(lambda x: tuple([int(el) for el in x.split('\\t')])).filter(lambda x: x[1] in aids)\n",
    "\n",
    "# now transform ratings\n",
    "# 255 means 'never play again' --> transform it to 1\n",
    "# then we have 101 possible scores ranging from 0...100\n",
    "# assign those to 2...102\n",
    "\n",
    "srdd = frdd.map(lambda x: (x[0], x[1], 1 if x[2] == 255 else 1+x[2]))    \n",
    "\n",
    "# save an rdd with its tuples to CSV (use chunk_size for merging of 16MB per default)\n",
    "def saveRDD2CSV(rdd, filename, chunk_size = 1024 * 1024 * 16):\n",
    "    tmppath = filename + '.tmp.dir'\n",
    "    # by default SPARK will create a folder \n",
    "    # to force to one part use .coalesce(1, shuffle=True)\n",
    "    # here we let spark do the distributed work and merge files later together\n",
    "    rdd.map(lambda x: ','.join(str(el) for el in x)).saveAsTextFile(tmppath)\n",
    "    \n",
    "    # only use the part files\n",
    "    sparkChunkFilter = lambda x: x[:4] == u'part'\n",
    "    # now merge the files in the folder together\n",
    "    # 16 MB chunks\n",
    "    with open(filename,'wb') as wfd:\n",
    "        for f in [el for el in os.listdir(tmppath) if sparkChunkFilter(el)]:\n",
    "            print 'merging ' + f + '...'\n",
    "            with open(tmppath + '/' + f,'rb') as fd:\n",
    "                shutil.copyfileobj(fd, wfd, chunk_size)\n",
    "            \n",
    "    \n",
    "    # remove tmp dir\n",
    "    shutil.rmtree(filename + '.tmp.dir')\n",
    "    \n",
    "# save adjusted rdd to file\n",
    "saveRDD2CSV(srdd, '../cache/musicdata.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Building an ordinal regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
