{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordinal Regression - Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to model categorical data there are two possible approaches:\n",
    "- multinomial logistic regression\n",
    "- ordinal regression\n",
    "\n",
    "### Multinomial logistic regression\n",
    "If the dependent variable $Y$ takes different discrete values, it can be modelled with an extension to the wide-known logistic regression model. Each category $c = c_1, ..., c_C$ is modelled via\n",
    "\n",
    "$$ \\mathbb{P}(Y_i = c_j) = \\frac{\\exp(\\beta_K X_i)}{1 + \\sum_{k=1}^K \\exp(\\beta_k X_i)}$$\n",
    "\n",
    "Both sklearn and Mlib implement this approach to some extent. I.e. sklearn can handle multinomial logistic regression, but uses a lbfgs or newton-cg approach only (no sgd) with support for L2 regularization solely. In Mlib \n",
    "\n",
    "However, multinomial logistic regression is not always the best model to choose. Consider i.e. the case of ratings. Here, the different categories represent ordinal values implying some kind of natural order. In a multinomial logistic regression model this order is neglected. I.e. rating '5' is as good as '4' and just another category.\n",
    "\n",
    "Links:\n",
    "- <http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html>\n",
    "- <http://spark.apache.org/docs/latest/mllib-linear-methods.html>\n",
    "- <http://de.slideshare.net/dbtsai/2014-0620-mlor-36132297>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import os\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from utils import loadCovertypeData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a description of the dataset see <https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.info>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "cache_path = os.path.join('..', 'cache')\n",
    "\n",
    "df = loadCovertypeData(os.path.join(cache_path, 'covtype.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>HDHyrdo</th>\n",
       "      <th>VDHydro</th>\n",
       "      <th>HDRoadways</th>\n",
       "      <th>9amHills</th>\n",
       "      <th>NoonHills</th>\n",
       "      <th>3pmHills</th>\n",
       "      <th>HDFirePoints</th>\n",
       "      <th>...</th>\n",
       "      <th>Soil_Type_32</th>\n",
       "      <th>Soil_Type_33</th>\n",
       "      <th>Soil_Type_34</th>\n",
       "      <th>Soil_Type_35</th>\n",
       "      <th>Soil_Type_36</th>\n",
       "      <th>Soil_Type_37</th>\n",
       "      <th>Soil_Type_38</th>\n",
       "      <th>Soil_Type_39</th>\n",
       "      <th>Soil_Type_40</th>\n",
       "      <th>Cover_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2596</td>\n",
       "      <td>51</td>\n",
       "      <td>3</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "      <td>510</td>\n",
       "      <td>221</td>\n",
       "      <td>232</td>\n",
       "      <td>148</td>\n",
       "      <td>6279</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2590</td>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>212</td>\n",
       "      <td>-6</td>\n",
       "      <td>390</td>\n",
       "      <td>220</td>\n",
       "      <td>235</td>\n",
       "      <td>151</td>\n",
       "      <td>6225</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2804</td>\n",
       "      <td>139</td>\n",
       "      <td>9</td>\n",
       "      <td>268</td>\n",
       "      <td>65</td>\n",
       "      <td>3180</td>\n",
       "      <td>234</td>\n",
       "      <td>238</td>\n",
       "      <td>135</td>\n",
       "      <td>6121</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2785</td>\n",
       "      <td>155</td>\n",
       "      <td>18</td>\n",
       "      <td>242</td>\n",
       "      <td>118</td>\n",
       "      <td>3090</td>\n",
       "      <td>238</td>\n",
       "      <td>238</td>\n",
       "      <td>122</td>\n",
       "      <td>6211</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2595</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>153</td>\n",
       "      <td>-1</td>\n",
       "      <td>391</td>\n",
       "      <td>220</td>\n",
       "      <td>234</td>\n",
       "      <td>150</td>\n",
       "      <td>6172</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Elevation  Aspect  Slope  HDHyrdo  VDHydro  HDRoadways  9amHills  \\\n",
       "0       2596      51      3      258        0         510       221   \n",
       "1       2590      56      2      212       -6         390       220   \n",
       "2       2804     139      9      268       65        3180       234   \n",
       "3       2785     155     18      242      118        3090       238   \n",
       "4       2595      45      2      153       -1         391       220   \n",
       "\n",
       "   NoonHills  3pmHills  HDFirePoints     ...      Soil_Type_32  Soil_Type_33  \\\n",
       "0        232       148          6279     ...                 0             0   \n",
       "1        235       151          6225     ...                 0             0   \n",
       "2        238       135          6121     ...                 0             0   \n",
       "3        238       122          6211     ...                 0             0   \n",
       "4        234       150          6172     ...                 0             0   \n",
       "\n",
       "   Soil_Type_34  Soil_Type_35  Soil_Type_36  Soil_Type_37  Soil_Type_38  \\\n",
       "0             0             0             0             0             0   \n",
       "1             0             0             0             0             0   \n",
       "2             0             0             0             0             0   \n",
       "3             0             0             0             0             0   \n",
       "4             0             0             0             0             0   \n",
       "\n",
       "   Soil_Type_39  Soil_Type_40  Cover_Type  \n",
       "0             0             0           5  \n",
       "1             0             0           5  \n",
       "2             0             0           2  \n",
       "3             0             0           2  \n",
       "4             0             0           5  \n",
       "\n",
       "[5 rows x 55 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split into variables\n",
    "respvar = ['Cover_Type']\n",
    "expvar = list(set(df.columns) - set(respvar))\n",
    "X = df[expvar]\n",
    "Y = df[respvar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Splitting into test and training sets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "itrain, itest = train_test_split(xrange(X.shape[0]), train_size=0.7)\n",
    "\n",
    "mask=np.ones(X.shape[0], dtype='int')\n",
    "mask[itrain]=1\n",
    "mask[itest]=0\n",
    "mask = (mask==1)\n",
    "\n",
    "X_train = np.array(X[mask])\n",
    "X_test = np.array(X[~mask])\n",
    "Y_train = np.array(Y[mask]).flatten()\n",
    "Y_test = np.array(Y[~mask]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sklearn multinomial logistic regression\n",
    "# set max_iter higher to increase accuracy. Leaving to default 100 needs ~ 1min\n",
    "clf = LogisticRegression(penalty='l2', multi_class='multinomial', solver='lbfgs', max_iter=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 13s, sys: 21.7 s, total: 1min 35s\n",
      "Wall time: 1min 5s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=200, multi_class='multinomial',\n",
       "          penalty='l2', random_state=None, solver='lbfgs', tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train train set\n",
    "%time clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for the sklearn implementation of the newton-cg / lbfgs solvers see https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/utils/optimize.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 82.4 ms, sys: 101 ms, total: 184 ms\n",
      "Wall time: 110 ms\n"
     ]
    }
   ],
   "source": [
    "# store class labels\n",
    "%time Y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for multinomial logistic regression: 64.4827427942\n"
     ]
    }
   ],
   "source": [
    "# compute accuracy\n",
    "accuracy = (1.0 * np.sum(Y_pred - Y_test == 0)) / Y_test.shape[0]\n",
    "print 'accuracy for multinomial logistic regression:', accuracy * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Ordinal regression\n",
    "To make up for this missing order, another model that is not yet implemented in sklearn nor Mlib is ordinal regression. The following is based mainly on Chapter9, Regression for Categorical Data, Gerhard Tutz. \n",
    "\n",
    "#### Model setup\n",
    "Suppose we are given a response captured by a random variable $Y \\in \\mathbb{R}$ which is categorial with w.l.o.g. (a mapping always exists) $K$ different categories $\\lbrace 1, ..., K \\rbrace $ with their natural order and furthermore\n",
    "$$ \\mathbb{P} ( Y \\in \\lbrace 1, ..., K \\rbrace) = 1$$\n",
    "\n",
    "We model the ordinal regression model using binary variables $y_1, ..., y_K$ (i.e. $y_k \\sim \\mathrm{Ber}(p_k) \\quad \\quad 0 \\leq p_k \\leq 1$) which will be used to transform an ordinal response variable into a binary response. This way, ordinal regression can be seen as an extension to a classical, binary variable model (such as logistic regression).\n",
    "\n",
    "For the transformation of the ordinal response to a binary one, there are three different, commonly used approaches(let $r =1, ..., K$):\n",
    "\n",
    "- Cumulative model:\n",
    "    $$ y_r = \\begin{cases}1 \\quad \\quad Y \\in \\lbrace 1, ..., r \\rbrace \\\\ 0 \\quad \\quad Y \\in \\lbrace r+1, ..., K\\rbrace\\end{cases}$$\n",
    "- Sequential model:\n",
    "    $$ y_r\\vert_{\\lbrace Y \\geq r \\rbrace} = \\begin{cases}1 \\quad \\quad Y =r \\vert Y \\geq r \\\\ 0 \\quad \\quad Y >r \\vert Y \\geq r \\end{cases}$$\n",
    "- Adjacent model:\n",
    "    $$ y_r\\vert_{\\lbrace Y \\in \\lbrace r, r+1\\rbrace \\rbrace} = \\begin{cases}1 \\quad \\quad Y = r \\vert Y \\in \\lbrace r, r+1\\rbrace \\\\ 0 \\quad \\quad Y = r+1 \\vert Y \\in \\lbrace r, r+1\\rbrace \\end{cases}$$ \n",
    "    \n",
    "### Cumulative model (Threshold model)\n",
    "The cumulative model (or threshold model) can be deviated assuming a latent, underlying r.v. $\\tilde{Y}$ for which the actual, observed variable $Y$ can be seen as somehow rounded.\n",
    "\n",
    "The link between those variables shall be modeled through\n",
    "$$ \\lbrace Y = r \\rbrace = \\lbrace \\theta_{r-1} < \\tilde{Y} \\leq \\theta_r \\rbrace$$\n",
    "for $-\\infty = \\theta_0 < \\theta_1 < ... < \\theta_{K-1} < \\theta_K = +\\infty$\n",
    "For the latent variable a linear regression model with $x, \\beta \\in \\mathbb{R^n}$ and a random variable $\\epsilon$ is choosen:\n",
    "\n",
    "$$ \\tilde{Y} = - x^T\\beta + \\epsilon $$\n",
    "It follows that\n",
    "$$ \\mathbb{P}(y_r = 1 \\vert x, \\beta) = \\mathbb{P}(Y \\leq r \\vert x, \\beta) = \\mathbb{P}(-x^T\\beta + \\epsilon \\leq \\theta_r) = \\mathbb{P}(\\epsilon \\leq \\theta_r + x^T\\beta) = F_\\epsilon(\\theta_r + x^T\\beta)$$\n",
    "\n",
    "$$ \\implies \\mathbb{P}(Y = r) = F_\\epsilon(\\theta_r + x^T\\beta) - F_\\epsilon(\\theta_{r-1} + x^T\\beta)$$\n",
    "using the cdf $F_\\epsilon$ of $\\epsilon$.\n",
    "\n",
    "In the model setup the only thing left to define is what distribution to assume/choose for $\\epsilon$. Some possible choices which yield different popular models are:\n",
    "\n",
    "- cumulative logit model: $$ \\epsilon \\sim \\mathrm{Logistic}(0,1)$$\n",
    "- probit model: $$ \\epsilon \\sim \\mathcal{N} (0, 1)$$\n",
    "- Maximum extreme-value model: $$ \\epsilon \\sim \\mathrm{Gumbel}(0, 1)$$\n",
    "- Minimum extreme-value model: $$ -\\epsilon \\sim \\mathrm{Gumbel}(0, 1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXMAAAECCAYAAAAMxDf2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njQXA/wdOVboK/eLJjwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b05cb90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pdfs for the different choices of epsilon\n",
    "%matplotlib inline\n",
    "from scipy.stats import logistic, norm, gumbel_l\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "fig.set_size_inches(6, 4, forward=True)\n",
    "x = np.linspace(-6, 6, 200)\n",
    "ax.plot(x, logistic.pdf(x), c=sns.color_palette()[0], lw=3, alpha=0.6, label='cumulative logit')\n",
    "ax.plot(x, norm.pdf(x), c=sns.color_palette()[1], lw=3, alpha=0.6, label='probit')\n",
    "ax.plot(x, gumbel_l.pdf(x), c=sns.color_palette()[2], lw=3, alpha=0.6, label='maximum extreme-value model')\n",
    "ax.plot(x, np.flipud(gumbel_l.pdf(x)), c=sns.color_palette()[4], lw=3, alpha=0.6, label='minimum extreme-value model')\n",
    "plt.legend()\n",
    "plt.ylim([0, 0.6])\n",
    "plt.savefig('epsdf.png', dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the model\n",
    "\n",
    "Assume we are given $n$ datapoints $(Y_1, X_1), ..., (Y_n, X_n)$ with $Y_i \\in \\lbrace 1, ..., K \\rbrace, \\quad X_i \\in \\mathbb{R}^m$ (independence assumption as usual). We simplify the fitting process, by using predefined boundaries $\\theta$. Of course it is also possible to put a prior on $\\theta$ and include it in the optimization approach but to do so, additional constraints need to be satisfied (i.e. $-\\infty = \\theta_0 < 1 < \\theta_1 < 2 < ... < \\theta_{K-1} < K < \\theta_K = \\infty$). One way to define the bounds is to use the middle, i.e.\n",
    "$\\theta_r = r - 0.5 \\quad \\text{for} \\quad r = 1, ..., K-1$. \n",
    "Then, the log likelihood function of the data $\\mathcal{D} = \\lbrace (Y_1, X_1), ..., (Y_n, X_n) \\rbrace$ is given as\n",
    "\n",
    "$$ \\mathcal{L}(\\beta \\; \\vert \\; \\mathcal{D}, \\theta) = \\sum_{i=1}^n \\log \\left( F_\\epsilon(\\theta_{Y_i} + X_i^T \\beta) - F_\\epsilon(\\theta_{Y_i-1} + X_i^T\\beta) \\right)$$\n",
    "\n",
    "### Maximum Likelihood Estimate\n",
    "this allows us to derive a Maximum Likelihood for the model\n",
    "\n",
    "$$ \\hat{\\beta}_{\\mathrm{MLE}} = \\underset{\\beta}{\\operatorname{argmax}} \\mathcal{L}(\\beta \\; \\vert \\; \\mathcal{D}, \\theta)$$\n",
    "### Maximum a posteriori estimate\n",
    "The MAP estimate can be derived by ignoring the normalization constant (cf. properties of an argmax) as \n",
    "$$ \\hat{\\beta}_{\\mathrm{MAP}} = \\underset{\\beta}{\\operatorname{argmax}} \\mathcal{L}(\\beta \\; \\vert \\; \\mathcal{D}, \\theta) + \\log f_\\beta(\\beta)$$\n",
    "\n",
    "with $f_\\beta$ being the pdf of the choosen prior distribution for $\\beta$. Popular choices are\n",
    "\n",
    "- Ridge regression: $\\beta \\sim \\mathcal{N}(0, \\tau I)\\quad \\quad$  (Gaussian prior)\n",
    "- LASSO regression: $\\beta_i \\sim \\mathrm{Laplace}(0, \\tau)\\quad \\quad$  (Laplace prior, indep. components)\n",
    "\n",
    "### Implementation notes\n",
    "One way to solve the optimization problems is to used parallelized stochastic gradient descent as described in\n",
    "<http://www.research.rutgers.edu/~lihong/pub/Zinkevich11Parallelized.pdf>. I.e. the data is distributed randomly over the nodes (the stoch. gradient shuffle step) and then per node the gradient is computed. To optimize the process, each node should node the same amount of data and the gradient is computed over all data. Thus the 'stochasticness' is done mainly in the shuffle step. After each round of computation results are aggregated by average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Gradient\n",
    "$$ \n",
    "\\begin{split}\n",
    "\\frac{\\partial \\mathcal{L}(\\beta \\;\\vert\\; \\mathcal{D}, \\theta )}{\\partial \\beta_j} &= \\frac{\\partial}{\\partial \\beta_j} \\sum_{i=1}^n \\log \\left( F_\\epsilon\\left( \\theta_{Y_i}  + X_i^T\\beta \\right) - F_\\epsilon\\left( \\theta_{Y_i - 1}  + X_i^T\\beta \\right) \\right) \\\\\n",
    "&= \\sum_{i=1}^n \\frac{\\partial}{\\partial \\beta_j} \\log \\left( F_\\epsilon\\left( \\theta_{Y_i}  + X_i^T\\beta \\right) - F_\\epsilon\\left( \\theta_{Y_i - 1}  + X_i^T\\beta \\right) \\right) \\\\\n",
    "&= \\sum_{i=1}^n  \\frac{\\frac{\\partial}{\\partial \\beta_j}\\left[ F_\\epsilon\\left( \\theta_{Y_i}  + X_i^T\\beta \\right) - F_\\epsilon\\left( \\theta_{Y_i - 1}  + X_i^T\\beta \\right) \\right]}{F_\\epsilon\\left( \\theta_{Y_i}  + X_i^T\\beta \\right) - F_\\epsilon\\left( \\theta_{Y_i - 1}  + X_i^T\\beta \\right)} \\\\\n",
    "&= \\sum_{i=1}^n  \\frac{f_\\epsilon\\left( \\theta_{Y_i}  + X_i^T\\beta \\right)X_{i,j} - f_\\epsilon\\left( \\theta_{Y_i - 1}  + X_i^T\\beta \\right)X_{i,j}}{F_\\epsilon\\left( \\theta_{Y_i}  + X_i^T\\beta \\right) - F_\\epsilon\\left( \\theta_{Y_i - 1}  + X_i^T\\beta \\right)} \\\\\n",
    "&= \\sum_{i=1}^n  \\frac{f_\\epsilon\\left( \\theta_{Y_i}  + X_i^T\\beta \\right) - f_\\epsilon\\left( \\theta_{Y_i - 1}  + X_i^T\\beta \\right)}{F_\\epsilon\\left( \\theta_{Y_i}  + X_i^T\\beta \\right) - F_\\epsilon\\left( \\theta_{Y_i - 1}  + X_i^T\\beta \\right)}X_{i,j} \\\\\n",
    "&= \\sum_{i=1}^n  g(\\theta, X_i, Y_i)X_{i,j} \\\\\n",
    "\\end{split}\n",
    "$$\n",
    "With $X_{i,j}$ being the $j$-th component of the vector $X_i$ and $f_\\epsilon$ being the density of $\\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr>\n",
    "<td>Logistic distribution</td>\n",
    "<td>$ \\mathrm{Logistic}(\\mu, s)$</td>\n",
    "<td>$$ F(x) = \\frac{1}{1 + \\exp(-\\frac{x-\\mu}{s})}$$</td>\n",
    "<td>$$ f(x) = \\frac{\\exp(-\\frac{x-\\mu}{s}}{s \\left( 1 + \\exp{- \\frac{x-\\mu}{s}}\\right)^2}$$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Normal distribution</td>\n",
    "<td>$ \\mathcal{N}(\\mu, \\sigma)$</td>\n",
    "<td>$$ F(x) = \\Phi\\left(\\frac{x - \\mu}{\\sigma}\\right)$$</td>\n",
    "<td>$$ f(x) = \\frac{1}{\\sigma}\\varphi\\left(\\frac{x - \\mu}{\\sigma}\\right)$$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Gumbel distribution (Maximum extreme value)</td>\n",
    "<td>$ \\mathrm{Gumbel}(\\mu, \\beta)$</td>\n",
    "<td>$$ F(x) = \\exp \\left( - \\exp \\left(- \\frac{x - \\mu}{\\beta} \\right) \\right)$$</td>\n",
    "<td>$$ f(x) = \\frac{1}{\\beta}\\exp \\left( - \\left( \\frac{x-\\mu}{\\beta} + \\exp \\left( \\frac{x - \\mu}{\\beta}\\right) \\right) \\right)$$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Flipped Gumbel distribution (Minimum extreme value)</td>\n",
    "<td>$ -\\mathrm{Gumbel}(\\mu, \\beta)$</td>\n",
    "<td>$$ F(x) = \\exp \\left( - \\exp \\left(- \\frac{-x - \\mu}{\\beta} \\right) \\right)$$</td>\n",
    "<td>$$ f(x) = \\frac{1}{\\beta}\\exp \\left( - \\left( \\frac{-x-\\mu}{\\beta} + \\exp \\left( \\frac{-x - \\mu}{\\beta}\\right) \\right) \\right)$$</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# implementation of the cumulative model (probit case)\n",
    "\n",
    "# negative log likelihood\n",
    "def nloglikelihood(X, y, theta, beta):\n",
    "    llsum = 0.\n",
    "    \n",
    "    for i in xrange(X.shape[0]):\n",
    "        llsum += np.log(norm.cdf(theta[y[i]]) - norm.cdf(theta[y[i]-1]))\n",
    "    \n",
    "    return -llsum\n",
    "\n",
    "# helper function g\n",
    "def gradient_g(x, yi, theta, beta):\n",
    "    g = norm.pdf(theta[yi] + np.dot(x, beta)) - norm.pdf(theta[yi - 1] + np.dot(x, beta)) / \\\n",
    "    (norm.cdf(theta[yi] + np.dot(x, beta)) - norm.cdf(theta[yi - 1] + np.dot(x, beta)))\n",
    "    return g\n",
    "\n",
    "# negative log likelihood gradient\n",
    "def nloglikelihood_gradient(X, y, theta, beta):\n",
    "    grad = 0\n",
    "    # more than one sample\n",
    "    if len(X.shape) > 1:\n",
    "        grad = np.zeros(X.shape[1])\n",
    "        for i in xrange(X.shape[0]):\n",
    "            grad += gradient_g(X[i, :], y[i], theta, beta)\n",
    "    else:\n",
    "        grad = gradient_g(X, y, theta, beta) * X\n",
    "            \n",
    "    return -grad\n",
    "    \n",
    "# stochastic gradient descent\n",
    "def sgd(X, y, theta, beta0, learning_rate = 0.05, num_epochs=1):\n",
    "    \n",
    "    beta = beta0.copy()\n",
    "    N = X.shape[0]\n",
    "    for epoch in xrange(num_epochs):\n",
    "        \n",
    "        # randomly shuffle data\n",
    "        permutation = np.random.choice(range(N), N, replace = False)\n",
    "        \n",
    "        # update using all ratings (minibatch size = 1)\n",
    "        for i in xrange(N):\n",
    "            # use here in a later version momentum\n",
    "            idx = permutation[i]\n",
    "            beta -= learning_rate * nloglikelihood(X[idx, :], y[idx], theta, beta)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Predicting the rating based on 'words'\n",
    "One possibility to predict a rating is to use actual contents. This could be for a the Jester joke dataset the actual joke with its wording, or given restaurant reviews its wording (i.e. relative word count). These can be either used as binary features (if some popular word exists) or as relative / absolute count.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Links:\n",
    "- <http://www.cambridge.org/de/academic/subjects/statistics-probability/statistical-theory-and-methods/regression-categorical-data>\n",
    "- <http://fa.bianp.net/blog/2013/logistic-ordinal-regression/>\n",
    "- <http://arxiv.org/pdf/1408.2327v6.pdf>\n",
    "- <http://www.stat.ufl.edu/~aa/ordinal/agresti_ordinal_tutorial.pdf>\n",
    "- <http://onlinelibrary.wiley.com/book/10.1002/9780470594001>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### A versatile model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, a general model is developed to model ratings. Therefore, the following variables are introduced\n",
    "<table>\n",
    "<tr>\n",
    "<td><b>variable</b></td><td><b>meaning</b></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>$N$</td><td>number of data samples $n=1, ..., N$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>$K$</td><td>number of latent variables per user and item</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>$L$</td><td>number of features describing each item</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>$I$</td><td>number of users, i.e. we have users $i = 1, ..., I$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>$J$</td><td>number of items, i.e. $j = 1, ..., J$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>$\\mathcal{D}$</td><td>data matrix consisting of $N$ rows $D_n$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>$\\mathcal{X}$</td><td>feature data matrix consisting of $J$ rows $X_n$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>$X_n$</td><td>$n$th data row, i.e $X_n = (X_{n, 1}, ..., , X_{n, L})^T$ with $X_{n,l} \\in \\mathbb{R}$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>$D_n$</td><td>$n$th data row, i.e $D_n = (r_n, i_n, j_n)^T$ with $r_n \\in \\lbrace 1, ..., R\\rbrace$ being a rating, $i_n, j_n$ being indices adressing a user and a item</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>$R$</td><td>number of ratings, i.e. each rating $r$ has to be $\\in \\lbrace 1, ..., R \\rbrace$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>$\\beta$</td><td>vector describing the buckets, i.e. $-\\infty = \\beta_0 < 1 < \\beta_1 < ... < \\beta_{R-1} < R < \\beta_R = \\infty$</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define now the model using the ordinal regression approach with a latent variable\n",
    "\n",
    "$$\\tilde{Y}_{i, j} = - \\left( u_i^Tv_j + X_j^Tw + a_i + b_j + g \\right) + \\epsilon$$\n",
    "Let $Y_n := Y_{i_n, j_n}$\n",
    "\n",
    "The model variables have the following definition/meaning \n",
    "<table>\n",
    "<tr>\n",
    "<td>$u_i \\in \\mathbb{R}^K$</td><td>latent variables of user $i$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>$v_j \\in \\mathbb{R}^K$</td><td>latent variables of item $j$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>$a_i \\in \\mathbb{R}$</td><td>bias for user $i$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>$b_j \\in \\mathbb{R}$</td><td>bias of item $j$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>$g \\in \\mathbb{R}$</td><td>global bias</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>$w \\in \\mathbb{R}^L$</td><td>weights for the $j$th item features</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Define $\\chi(i,j) := u_i^Tv_j + X_j^Tw + a_i + b_j + g$ and $\\chi_n := \\chi(i_n, j_n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full model is parametrized via\n",
    "$$\\theta = \\begin{pmatrix} u_1 & ... & u_I & v_1 & ... & v_J & a_1 & ... & a_I & b_1 & ... & b_J & g & w \\end{pmatrix}^T$$\n",
    "\n",
    "We can write the loglikelihood as\n",
    "\n",
    "$$\\mathcal{L}(\\theta \\,\\vert\\, \\mathcal{D}, \\mathcal{X}, \\beta) = \\sum_{n=1}^N \\log \\left( F_\\epsilon(\\beta_{Y_n} + \\chi_n) - F_\\epsilon(\\beta_{Y_n - 1} + \\chi_n) \\right)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Computing the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient can be computed easily via the standard rules\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial}{\\partial u_i} \\mathcal{L}(\\theta \\,\\vert\\, \\mathcal{D}, \\mathcal{X}, \\beta) &= \\sum_{n=1}^N \\frac{\\partial}{\\partial u_i} \\log \\left( F_\\epsilon(\\beta_{Y_n} + \\chi_n) - F_\\epsilon(\\beta_{Y_n - 1} + \\chi_n) \\right) \\\\\n",
    "&= \\sum_{n \\in \\lbrace n =1, ..., N \\;\\vert\\; i_n = i\\rbrace} \\frac{\\partial}{\\partial u_i} \\log \\left( F_\\epsilon(\\beta_{Y_n} + \\chi_n) - F_\\epsilon(\\beta_{Y_n - 1} + \\chi_n) \\right)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Let $I_i := \\lbrace n =1, ..., N \\;\\vert\\; i_n = i\\rbrace, \\quad J_j := \\lbrace n =1, ..., N \\;\\vert\\; j_n = j\\rbrace$ , then\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial}{\\partial u_i} \\mathcal{L}(\\theta \\,\\vert\\, \\mathcal{D}, \\mathcal{X}, \\beta) &= \\sum_{n \\in I_i}  \\frac{ \\frac{\\partial}{\\partial u_i}\\left( F_\\epsilon(\\beta_{Y_n} + \\chi_n) - F_\\epsilon(\\beta_{Y_n - 1} + \\chi_n) \\right)}{F_\\epsilon(\\beta_{Y_n} + \\chi_n) - F_\\epsilon(\\beta_{Y_n - 1} + \\chi_n)} \\\\\n",
    "&= \\sum_{n \\in I_i}  \\frac{ f_\\epsilon(\\beta_{Y_n} + \\chi_n) - f_\\epsilon(\\beta_{Y_n - 1} + \\chi_n)}{F_\\epsilon(\\beta_{Y_n} + \\chi_n) - F_\\epsilon(\\beta_{Y_n - 1} + \\chi_n)}\\frac{\\partial}{\\partial u_i} \\chi_n\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure will be always the same, so define\n",
    "$$ q_n := \\frac{ f_\\epsilon(\\beta_{Y_n} + \\chi_n) - f_\\epsilon(\\beta_{Y_n - 1} + \\chi_n)}{F_\\epsilon(\\beta_{Y_n} + \\chi_n) - F_\\epsilon(\\beta_{Y_n - 1} + \\chi_n)}$$\n",
    "\n",
    "the gradient of any $\\xi$ of the individual components can now be written as\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\xi} \\mathcal{L}(\\theta \\,\\vert\\, \\mathcal{D}, \\mathcal{X}, \\beta) = \\sum_{n=1}^N q_n \\frac{\\partial}{\\partial \\xi}\\chi_n$$\n",
    "(note that for many terms this is zero!)\n",
    "\n",
    "Given\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial u_i}\\chi_n = \\begin{cases} v_{j_n} \\quad n \\in I_i \\\\\n",
    "0 \\quad \\text{else}\\end{cases}$$\n",
    "$$ \\frac{\\partial}{\\partial v_j}\\chi_n = \\begin{cases} u_{i_n} \\quad n \\in J_j \\\\\n",
    "0 \\quad \\text{else}\\end{cases}$$\n",
    "$$ \\frac{\\partial}{\\partial a_i}\\chi_n = \\begin{cases} 1 \\quad n \\in I_i \\\\\n",
    "0 \\quad \\text{else}\\end{cases}$$\n",
    "$$ \\frac{\\partial}{\\partial b_j}\\chi_n = \\begin{cases} 1 \\quad n \\in J_j \\\\\n",
    "0 \\quad \\text{else}\\end{cases}$$\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial w}\\chi_n = X_{j_n}$$\n",
    "$$\\frac{\\partial}{\\partial g}\\chi_n = 1$$\n",
    "\n",
    "the gradient of the full model is\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\theta} \\mathcal{L}(\\theta \\,\\vert\\, \\mathcal{D}, \\mathcal{X}, \\beta) = \\begin{pmatrix} \\sum_{n \\in I_1}q_n v_{j_n} & ... & \\sum_{n \\in I_I}q_n v_{j_n} & \\sum_{n \\in J_1}q_n u_{i_n} & ... & \\sum_{n \\in J_J}q_n u_{i_n} & \\sum_{n \\in I_1}q_n & ... & \\sum_{n \\in I_I}q_n & \\sum_{n \\in J_1}q_n & ... & \\sum_{n \\in J_J}q_n & \\sum_{n =1, ..., N}q_n & \\sum_{n =1, ..., N}q_nX_{j_n} \\end{pmatrix}^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
